{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tracing and Evaluating a RAG Pipeline with Netra\n",
        "\n",
        "This notebook walks you through adding full observability and systematic evaluation to a Retrieval-Augmented Generation (RAG) pipeline using Netraâ€”tracing every stage from document ingestion to answer generation.\n",
        "\n",
        "What You'll Learn\n",
        "Build a RAG pipeline that processes PDFs and answers questions.\n",
        "\n",
        "Trace every stage: chunking, embedding, retrieval, and generation.\n",
        "\n",
        "Track token usage, costs, and latency per query.\n",
        "\n",
        "Evaluate retrieval quality, answer correctness, and faithfulness.\n",
        "\n",
        "Prerequisites\n",
        "\n",
        "OpenAI API key\n",
        "\n",
        "Netra API key ([Steps Mentioned here](https://docs.getnetra.ai/quick-start/Overview))"
      ],
      "metadata": {
        "id": "86J6nG5hPWie"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step0"
      },
      "source": [
        "---\n",
        "## Step 0: Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "pip install netra-sdk openai chromadb pypdf reportlab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "---\n",
        "## Step 1: Set Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_vars"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key:\")\n",
        "os.environ[\"NETRA_API_KEY\"] = getpass(\"Enter your Netra API Key:\")\n",
        "os.environ[\"NETRA_OTLP_ENDPOINT\"] = getpass(\"Enter your Netra OTLP Endpoint:\")\n",
        "\n",
        "print(\"API keys configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "---\n",
        "## Step 2: Initialize Netra for Observability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_netra"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from netra import Netra, SpanType, UsageModel\n",
        "\n",
        "print(os.getenv('NETRA_API_KEY'))\n",
        "\n",
        "Netra.init(\n",
        "    app_name=\"pdf-qa-chatbot\",\n",
        "    headers=f\"x-api-key={os.getenv('NETRA_API_KEY')}\",\n",
        "    environment=\"development\",\n",
        "    trace_content=True,\n",
        ")\n",
        "\n",
        "print(\"Netra initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "---\n",
        "## Step 3: Import Libraries and Initialize Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from typing import List, Optional\n",
        "from pypdf import PdfReader\n",
        "import chromadb\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Initialize clients\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "print(\"Clients initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "---\n",
        "## Step 4: Define Helper Functions\n",
        "\n",
        "These functions handle PDF loading, text chunking, and embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helpers"
      },
      "outputs": [],
      "source": [
        "def load_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def generate_embeddings(texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
        "    response = openai_client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=texts\n",
        "    )\n",
        "    return [item.embedding for item in response.data]\n",
        "\n",
        "\n",
        "print(\"Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "---\n",
        "## Step 5: Create Traced Ingestion Pipeline\n",
        "\n",
        "This function ingests a PDF with full tracing for each stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ingestion"
      },
      "outputs": [],
      "source": [
        "def ingest_pdf_traced(file_path: str, collection_name: str = \"pdf_qa\") -> dict:\n",
        "    \"\"\"Ingest a PDF with full tracing.\"\"\"\n",
        "    with Netra.start_span(\"pdf-ingestion\") as parent_span:\n",
        "        parent_span.set_attribute(\"pdf.path\", file_path)\n",
        "\n",
        "        # Step 1: Load PDF\n",
        "        with Netra.start_span(\"load-pdf\") as load_span:\n",
        "            pdf_text = load_pdf(file_path)\n",
        "            load_span.set_attribute(\"pdf.characters\", len(pdf_text))\n",
        "            load_span.set_success()\n",
        "            print(f\"Loaded PDF: {len(pdf_text)} characters\")\n",
        "\n",
        "        # Step 2: Chunk text\n",
        "        with Netra.start_span(\"chunk-text\") as chunk_span:\n",
        "            chunks = chunk_text(pdf_text, chunk_size=1000, overlap=200)\n",
        "            chunk_span.set_attribute(\"chunks.count\", len(chunks))\n",
        "            chunk_span.set_attribute(\"chunks.avg_size\", len(pdf_text) // len(chunks))\n",
        "            chunk_span.set_success()\n",
        "            print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "        # Step 3: Generate embeddings\n",
        "        with Netra.start_span(\"generate-embeddings\", as_type=SpanType.EMBEDDING) as embed_span:\n",
        "            embed_span.set_model(\"text-embedding-3-small\")\n",
        "            embed_span.set_llm_system(\"openai\")\n",
        "\n",
        "            embeddings = generate_embeddings(chunks)\n",
        "\n",
        "            # Track embedding costs (approximate)\n",
        "            total_tokens = sum(len(chunk.split()) * 1.3 for chunk in chunks)\n",
        "            embed_span.set_usage([\n",
        "                UsageModel(\n",
        "                    model=\"text-embedding-3-small\",\n",
        "                    cost_in_usd=total_tokens * 0.00002 / 1000,\n",
        "                    usage_type=\"embedding\",\n",
        "                    units_used=int(total_tokens)\n",
        "                )\n",
        "            ])\n",
        "            embed_span.set_success()\n",
        "            print(f\"Generated {len(embeddings)} embeddings\")\n",
        "\n",
        "        # Step 4: Store in vector DB\n",
        "        with Netra.start_span(\"store-vectors\") as store_span:\n",
        "            # Delete collection if exists\n",
        "            try:\n",
        "                chroma_client.delete_collection(name=collection_name)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            collection = chroma_client.create_collection(name=collection_name)\n",
        "            collection.add(\n",
        "                documents=chunks,\n",
        "                embeddings=embeddings,\n",
        "                ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "            )\n",
        "            store_span.set_attribute(\"vectors.count\", len(chunks))\n",
        "            store_span.set_attribute(\"collection.name\", collection_name)\n",
        "            store_span.set_success()\n",
        "            print(f\"Stored {len(chunks)} vectors in ChromaDB\")\n",
        "\n",
        "        parent_span.set_attribute(\"ingestion.chunks_created\", len(chunks))\n",
        "        parent_span.set_success()\n",
        "\n",
        "        return {\n",
        "            \"chunks_count\": len(chunks),\n",
        "            \"collection\": collection,\n",
        "            \"chunks\": chunks\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Ingestion function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "---\n",
        "## Step 6: Create Traced Retrieval Function\n",
        "\n",
        "This function retrieves relevant chunks with similarity score tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "retrieval"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks_traced(query: str, collection, top_k: int = 3) -> List[dict]:\n",
        "    \"\"\"Retrieve chunks with full tracing.\"\"\"\n",
        "    with Netra.start_span(\"retrieval\", as_type=SpanType.TOOL) as span:\n",
        "        span.set_attribute(\"query\", query)\n",
        "        span.set_attribute(\"top_k\", top_k)\n",
        "\n",
        "        # Generate query embedding\n",
        "        with Netra.start_span(\"query-embedding\", as_type=SpanType.EMBEDDING) as embed_span:\n",
        "            embed_span.set_model(\"text-embedding-3-small\")\n",
        "            query_embedding = generate_embeddings([query])[0]\n",
        "            embed_span.set_success()\n",
        "\n",
        "        # Perform vector search\n",
        "        with Netra.start_span(\"vector-search\") as search_span:\n",
        "            results = collection.query(\n",
        "                query_embeddings=[query_embedding],\n",
        "                n_results=top_k,\n",
        "                include=[\"documents\", \"distances\"]\n",
        "            )\n",
        "            search_span.set_attribute(\"results.count\", len(results[\"documents\"][0]))\n",
        "            search_span.set_success()\n",
        "\n",
        "        # Process results\n",
        "        retrieved = []\n",
        "        for i, doc in enumerate(results[\"documents\"][0]):\n",
        "            similarity = 1 - results[\"distances\"][0][i]\n",
        "            retrieved.append({\n",
        "                \"content\": doc,\n",
        "                \"similarity_score\": similarity,\n",
        "                \"chunk_id\": f\"chunk_{i}\"\n",
        "            })\n",
        "\n",
        "        # Log retrieval quality metrics\n",
        "        if retrieved:\n",
        "            span.set_attribute(\"retrieval.avg_similarity\",\n",
        "                             sum(r[\"similarity_score\"] for r in retrieved) / len(retrieved))\n",
        "            span.set_attribute(\"retrieval.max_similarity\",\n",
        "                             max(r[\"similarity_score\"] for r in retrieved))\n",
        "            span.set_attribute(\"retrieval.min_similarity\",\n",
        "                             min(r[\"similarity_score\"] for r in retrieved))\n",
        "\n",
        "        span.add_event(\"chunks-retrieved\", {\n",
        "            \"count\": len(retrieved),\n",
        "            \"similarity_scores\": [r[\"similarity_score\"] for r in retrieved]\n",
        "        })\n",
        "\n",
        "        span.set_success()\n",
        "        return retrieved\n",
        "\n",
        "\n",
        "print(\"Retrieval function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "---\n",
        "## Step 7: Create Traced Answer Generation Function\n",
        "\n",
        "This function generates answers with token and cost tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generation"
      },
      "outputs": [],
      "source": [
        "def generate_answer_traced(query: str, context_chunks: List[dict]) -> dict:\n",
        "    \"\"\"Generate answer with full tracing and cost tracking.\"\"\"\n",
        "    with Netra.start_span(\"answer-generation\", as_type=SpanType.GENERATION) as span:\n",
        "        span.set_model(\"gpt-4o-mini\")\n",
        "        span.set_llm_system(\"openai\")\n",
        "\n",
        "        context = \"\\n\\n\".join([chunk[\"content\"] for chunk in context_chunks])\n",
        "\n",
        "        prompt = f\"\"\"Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer the question based only on the provided context. If the answer is not in the context, say \"I cannot find this information in the document.\"\n",
        "\"\"\"\n",
        "\n",
        "        span.set_prompt(prompt)\n",
        "        span.set_attribute(\"context.chunks_count\", len(context_chunks))\n",
        "        span.set_attribute(\"context.total_chars\", len(context))\n",
        "\n",
        "        span.add_event(\"generation-started\")\n",
        "\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful assistant that answers questions based on provided context.\"\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        # Track token usage and cost\n",
        "        prompt_tokens = response.usage.prompt_tokens\n",
        "        completion_tokens = response.usage.completion_tokens\n",
        "\n",
        "        # GPT-4o-mini pricing: $0.15/1M input, $0.60/1M output\n",
        "        cost = (prompt_tokens * 0.00015 / 1000) + (completion_tokens * 0.0006 / 1000)\n",
        "\n",
        "        span.set_usage([\n",
        "            UsageModel(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                cost_in_usd=cost,\n",
        "                usage_type=\"chat\",\n",
        "                units_used=prompt_tokens + completion_tokens\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        span.set_attribute(\"tokens.prompt\", prompt_tokens)\n",
        "        span.set_attribute(\"tokens.completion\", completion_tokens)\n",
        "        span.set_attribute(\"tokens.total\", prompt_tokens + completion_tokens)\n",
        "        span.set_attribute(\"cost.usd\", cost)\n",
        "\n",
        "        span.add_event(\"generation-completed\", {\n",
        "            \"answer_length\": len(answer),\n",
        "            \"tokens_used\": prompt_tokens + completion_tokens\n",
        "        })\n",
        "\n",
        "        span.set_success()\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"token_usage\": {\n",
        "                \"prompt\": prompt_tokens,\n",
        "                \"completion\": completion_tokens,\n",
        "                \"total\": prompt_tokens + completion_tokens\n",
        "            },\n",
        "            \"cost_usd\": cost\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Answer generation function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "---\n",
        "## Step 8: Create the Complete Traced Chatbot Class\n",
        "\n",
        "This class combines all the traced functions into a complete chatbot with session support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chatbot_class"
      },
      "outputs": [],
      "source": [
        "class TracedPDFChatbot:\n",
        "    \"\"\"RAG Pipeline with full Netra tracing.\"\"\"\n",
        "\n",
        "    def __init__(self, pdf_path: str):\n",
        "        self.pdf_path = pdf_path\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "        self.collection = None\n",
        "        self.chunks = []\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initialize with tracing.\"\"\"\n",
        "        print(f\"Initializing chatbot for: {self.pdf_path}\")\n",
        "        print(f\"Session ID: {self.session_id}\")\n",
        "\n",
        "        result = ingest_pdf_traced(self.pdf_path, f\"pdf_{self.session_id[:8]}\")\n",
        "        self.collection = result[\"collection\"]\n",
        "        self.chunks = result[\"chunks\"]\n",
        "\n",
        "        print(f\"\\nInitialization complete! {result['chunks_count']} chunks indexed.\")\n",
        "\n",
        "    def chat(self, query: str, user_id: str = None) -> dict:\n",
        "        \"\"\"Process a chat message with full tracing.\"\"\"\n",
        "        # Set session and user context\n",
        "        Netra.set_session_id(self.session_id)\n",
        "        if user_id:\n",
        "            Netra.set_user_id(user_id)\n",
        "\n",
        "        with Netra.start_span(\"pdf-qa-query\", as_type=SpanType.AGENT) as span:\n",
        "            span.set_attribute(\"query\", query)\n",
        "            span.set_attribute(\"session_id\", self.session_id)\n",
        "            if user_id:\n",
        "                span.set_attribute(\"user_id\", user_id)\n",
        "\n",
        "            # Retrieve relevant chunks\n",
        "            retrieved_chunks = retrieve_chunks_traced(query, self.collection)\n",
        "\n",
        "            # Generate answer\n",
        "            result = generate_answer_traced(query, retrieved_chunks)\n",
        "\n",
        "            # Update conversation history\n",
        "            self.conversation_history.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": query\n",
        "            })\n",
        "            self.conversation_history.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": result[\"answer\"]\n",
        "            })\n",
        "\n",
        "            span.set_attribute(\"response.length\", len(result[\"answer\"]))\n",
        "            span.set_attribute(\"cost.total_usd\", result[\"cost_usd\"])\n",
        "            span.set_success()\n",
        "\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"answer\": result[\"answer\"],\n",
        "                \"retrieved_chunks\": retrieved_chunks,\n",
        "                \"token_usage\": result[\"token_usage\"],\n",
        "                \"cost_usd\": result[\"cost_usd\"],\n",
        "                \"session_id\": self.session_id\n",
        "            }\n",
        "\n",
        "\n",
        "print(\"TracedPDFChatbot class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "---\n",
        "## Step 9: Upload or Create a Sample PDF\n",
        "\n",
        "Upload your own PDF or create a sample one for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_pdf"
      },
      "outputs": [],
      "source": [
        "# Option 1: Upload your own PDF (uncomment to use)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Option 2: Create a sample PDF for testing\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "def create_sample_pdf(filename: str = \"sample_document.pdf\"):\n",
        "    \"\"\"Create a sample PDF for testing.\"\"\"\n",
        "    c = canvas.Canvas(filename, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    # Page 1: Introduction\n",
        "    c.setFont(\"Helvetica-Bold\", 24)\n",
        "    c.drawString(100, height - 100, \"Introduction to Machine Learning\")\n",
        "\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    text = \"\"\"\n",
        "    Machine learning is a subset of artificial intelligence that enables systems to learn\n",
        "    and improve from experience without being explicitly programmed. The term was coined\n",
        "    by Arthur Samuel in 1959 while at IBM.\n",
        "\n",
        "    Machine learning algorithms build a mathematical model based on sample data, known as\n",
        "    training data, in order to make predictions or decisions without being explicitly\n",
        "    programmed to perform the task.\n",
        "\n",
        "    The primary aim is to allow computers to learn automatically without human intervention\n",
        "    or assistance and adjust actions accordingly.\n",
        "    \"\"\"\n",
        "\n",
        "    y = height - 150\n",
        "    for line in text.strip().split(\"\\n\"):\n",
        "        c.drawString(100, y, line.strip())\n",
        "        y -= 20\n",
        "\n",
        "    # Page 2: Types of ML\n",
        "    c.showPage()\n",
        "    c.setFont(\"Helvetica-Bold\", 18)\n",
        "    c.drawString(100, height - 100, \"Types of Machine Learning\")\n",
        "\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    text = \"\"\"\n",
        "    There are three main types of machine learning:\n",
        "\n",
        "    1. Supervised Learning: The algorithm learns from labeled training data and makes\n",
        "       predictions based on that data. Examples include classification and regression.\n",
        "\n",
        "    2. Unsupervised Learning: The algorithm learns patterns from unlabeled data without\n",
        "       any guidance. Examples include clustering and dimensionality reduction.\n",
        "\n",
        "    3. Reinforcement Learning: The algorithm learns through interaction with an environment,\n",
        "       receiving rewards or penalties for actions. Used in robotics and game playing.\n",
        "\n",
        "    Each type has its own applications and is suited for different kinds of problems.\n",
        "    \"\"\"\n",
        "\n",
        "    y = height - 150\n",
        "    for line in text.strip().split(\"\\n\"):\n",
        "        c.drawString(100, y, line.strip())\n",
        "        y -= 20\n",
        "\n",
        "    # Page 3: Applications\n",
        "    c.showPage()\n",
        "    c.setFont(\"Helvetica-Bold\", 18)\n",
        "    c.drawString(100, height - 100, \"Applications of Machine Learning\")\n",
        "\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    text = \"\"\"\n",
        "    Machine learning has numerous real-world applications:\n",
        "\n",
        "    - Image Recognition: Identifying objects, faces, and scenes in images\n",
        "    - Natural Language Processing: Translation, sentiment analysis, chatbots\n",
        "    - Recommendation Systems: Netflix, Amazon, Spotify recommendations\n",
        "    - Autonomous Vehicles: Self-driving cars use ML for navigation\n",
        "    - Healthcare: Disease diagnosis, drug discovery, personalized treatment\n",
        "    - Financial Services: Fraud detection, algorithmic trading, credit scoring\n",
        "\n",
        "    The field continues to grow rapidly with new applications emerging regularly.\n",
        "    \"\"\"\n",
        "\n",
        "    y = height - 150\n",
        "    for line in text.strip().split(\"\\n\"):\n",
        "        c.drawString(100, y, line.strip())\n",
        "        y -= 20\n",
        "\n",
        "    c.save()\n",
        "    print(f\"Created sample PDF: {filename}\")\n",
        "    return filename\n",
        "\n",
        "# Create the sample PDF\n",
        "pdf_path = create_sample_pdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "---\n",
        "## Step 10: Initialize and Test the Chatbot\n",
        "\n",
        "Now let's create and test our traced PDF chatbot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_chatbot"
      },
      "outputs": [],
      "source": [
        "# Initialize the chatbot\n",
        "chatbot = TracedPDFChatbot(pdf_path)\n",
        "chatbot.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTJM53YaMrTw"
      },
      "outputs": [],
      "source": [
        "response = chatbot.chat(\n",
        "    \"What are the key findings?\",\n",
        "    user_id=\"user-123\"\n",
        ")\n",
        "print(f\"Answer: {response['answer']}\")\n",
        "print(f\"Cost: ${response['cost_usd']:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_chatbot"
      },
      "outputs": [],
      "source": [
        "# Test with a few questions\n",
        "questions = [\n",
        "    \"What is machine learning?\",\n",
        "    \"What are the three types of machine learning?\",\n",
        "    \"Who coined the term machine learning and when?\",\n",
        "    \"What are some applications of machine learning?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing the RAG Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    print(f\"\\n--- Question {i} ---\")\n",
        "    print(f\"Q: {question}\")\n",
        "\n",
        "    response = chatbot.chat(question, user_id=\"test-user\")\n",
        "\n",
        "    print(f\"\\nA: {response['answer']}\")\n",
        "    print(f\"\\n[Tokens: {response['token_usage']['total']} | Cost: ${response['cost_usd']:.6f}]\")\n",
        "    print(f\"[Retrieved {len(response['retrieved_chunks'])} chunks | \"\n",
        "          f\"Max similarity: {max(c['similarity_score'] for c in response['retrieved_chunks']):.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step12"
      },
      "source": [
        "---\n",
        "## Step 12: Run Evaluation\n",
        "\n",
        "Run the chatbot against all test cases and collect results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_eval"
      },
      "outputs": [],
      "source": [
        "def run_evaluation(dataset_id: str = \"local-eval\"):\n",
        "    \"\"\"Run evaluation on all test cases.\"\"\"\n",
        "    dataset = Netra.evaluation.get_dataset(dataset_id)\n",
        "    Netra.evaluation.run_test_suite(\n",
        "        name=\"Evaluation Test Demo\",\n",
        "        data=dataset,\n",
        "        task=lambda eval_input: print(eval_input) # Define a function based on your evaluation needs! The supplied function is called with the evaluator input as defind in the dataset\n",
        "    )\n",
        "\n",
        "# Run evaluation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Running Evaluation\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "eval_results = run_evaluation(\"replace the id with your netra dataset id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step14"
      },
      "source": [
        "---\n",
        "## Step 14: Shutdown Netra\n",
        "\n",
        "Flush all pending traces and gracefully shutdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shutdown"
      },
      "outputs": [],
      "source": [
        "# Shutdown Netra gracefully\n",
        "Netra.shutdown()\n",
        "print(\"Netra shutdown complete!\")\n",
        "print(\"\\nView your traces at: https://app.getnetra.ai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "---\n",
        "## View Details on Evaluations and Test Runs\n",
        "\n",
        "For more details on how to do evaluations and test runs on this RAG project,  \n",
        "refer to the [Tracing and Evaluating a RAG Pipeline Cookbook](https://docs.getnetra.ai/Cookbooks/pdf-qa-rag-chatbot).\n",
        "\n",
        "## Documentation Links\n",
        "\n",
        "- [Netra Documentation](https://docs.getnetra.ai)\n",
        "- [Evaluators Guide](https://docs.getnetra.ai/Evaluation/Evaluators)\n",
        "- [Datasets Guide](https://docs.getnetra.ai/Evaluation/Datasets)\n",
        "- [Manual Tracing](https://docs.getnetra.ai/Observability/Traces/manual-tracing)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}