{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tracing a RAG Pipeline\n",
    "\n",
    "This notebook walks you through adding **full observability** to a Retrieval-Augmented Generation (RAG) pipeline—tracing every stage from document ingestion to answer generation, tracking costs, and monitoring performance.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Build a complete RAG chatbot that loads PDFs, chunks documents, generates embeddings, and retrieves relevant context\n",
    "- Add auto-instrumentation to trace every stage—chunking, embedding, retrieval, and generation\n",
    "- Monitor token usage, API costs, and latency at each step to identify bottlenecks\n",
    "- Track usage per user and session to understand conversation flows and user behavior\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python >=3.10, <3.14\n",
    "- OpenAI API key\n",
    "- Netra API key ([Steps mentioned here](https://docs.getnetra.ai/quick-start/Overview))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 0: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install netra-sdk openai chromadb pypdf reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"NETRA_API_KEY\"] = getpass(\"Enter your Netra API Key:\")\n",
    "os.environ[\"NETRA_OTLP_ENDPOINT\"] = getpass(\"Enter your Netra OTLP Endpoint:\")\n",
    "\n",
    "print(\"API keys configured!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Netra for Observability\n",
    "\n",
    "With auto-instrumentation, Netra automatically captures all OpenAI and ChromaDB operations—no decorators or manual spans required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from netra import Netra\n",
    "from netra.instrumentation.instruments import InstrumentSet\n",
    "\n",
    "Netra.init(\n",
    "    app_name=\"pdf-qa-chatbot\",\n",
    "    headers=f\"x-api-key={os.getenv('NETRA_API_KEY')}\",\n",
    "    environment=\"development\",\n",
    "    trace_content=True,\n",
    "    instruments={\n",
    "        InstrumentSet.OPENAI,\n",
    "        InstrumentSet.CHROMA,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Netra initialized with auto-instrumentation!\")\n",
    "print(\"All OpenAI and ChromaDB operations will be traced automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries and Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import List, Dict, Optional\n",
    "from pypdf import PdfReader\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "print(\"Clients initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Define Helper Functions\n",
    "\n",
    "These functions handle PDF loading, text chunking, and embedding generation. With auto-instrumentation, the `generate_embeddings()` call is automatically traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a list of texts.\n",
    "\n",
    "    This call is automatically traced by Netra, capturing:\n",
    "    - Model used (text-embedding-3-small)\n",
    "    - Token count\n",
    "    - Latency\n",
    "    \"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=texts\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Define Ingestion, Retrieval, and Generation Functions\n",
    "\n",
    "These core RAG functions are automatically traced—no manual instrumentation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_pdf(file_path: str, collection_name: str = \"pdf_qa\") -> dict:\n",
    "    \"\"\"Ingest a PDF into the vector database.\n",
    "\n",
    "    Auto-traced operations:\n",
    "    - OpenAI embeddings.create() call\n",
    "    - ChromaDB collection.add() call\n",
    "    \"\"\"\n",
    "    # Load and chunk PDF\n",
    "    pdf_text = load_pdf(file_path)\n",
    "    chunks = chunk_text(pdf_text, chunk_size=1000, overlap=200)\n",
    "    print(f\"Loaded PDF: {len(pdf_text)} characters, {len(chunks)} chunks\")\n",
    "\n",
    "    # Generate embeddings (auto-traced)\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "    print(f\"Generated {len(embeddings)} embeddings\")\n",
    "\n",
    "    # Store in vector database (auto-traced)\n",
    "    try:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    )\n",
    "    print(f\"Stored {len(chunks)} vectors in ChromaDB\")\n",
    "\n",
    "    return {\n",
    "        \"chunks_count\": len(chunks),\n",
    "        \"collection\": collection,\n",
    "        \"chunks\": chunks\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_chunks(query: str, collection, top_k: int = 3) -> List[dict]:\n",
    "    \"\"\"Retrieve relevant chunks for a query.\n",
    "\n",
    "    Auto-traced operations:\n",
    "    - OpenAI embeddings.create() for query\n",
    "    - ChromaDB collection.query() call\n",
    "    \"\"\"\n",
    "    # Generate query embedding (auto-traced)\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "\n",
    "    # Perform vector search (auto-traced)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    retrieved = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        similarity = 1 - results[\"distances\"][0][i]\n",
    "        retrieved.append({\n",
    "            \"content\": doc,\n",
    "            \"similarity_score\": similarity,\n",
    "            \"chunk_id\": f\"chunk_{i}\"\n",
    "        })\n",
    "\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "def generate_answer(query: str, context_chunks: List[dict]) -> dict:\n",
    "    \"\"\"Generate an answer using the retrieved context.\n",
    "\n",
    "    Auto-traced operations:\n",
    "    - OpenAI chat.completions.create() with full token/cost tracking\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([chunk[\"content\"] for chunk in context_chunks])\n",
    "\n",
    "    # Generate answer (auto-traced with model, tokens, cost, latency)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "                Only use information from the context to answer. If the answer is not in the context, say so.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"token_usage\": {\n",
    "            \"prompt\": response.usage.prompt_tokens,\n",
    "            \"completion\": response.usage.completion_tokens,\n",
    "            \"total\": response.usage.total_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"RAG functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Create the PDFChatbot Class\n",
    "\n",
    "This class combines all the functions into a complete chatbot with session and user tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFChatbot:\n",
    "    \"\"\"RAG Pipeline with Netra auto-instrumentation.\"\"\"\n",
    "\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.collection = None\n",
    "        self.chunks: List[str] = []\n",
    "        self.conversation_history: List[Dict] = []\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the vector store with PDF content.\"\"\"\n",
    "        print(f\"Initializing chatbot for: {self.pdf_path}\")\n",
    "        print(f\"Session ID: {self.session_id}\")\n",
    "\n",
    "        result = ingest_pdf(self.pdf_path, f\"pdf_{self.session_id[:8]}\")\n",
    "        self.collection = result[\"collection\"]\n",
    "        self.chunks = result[\"chunks\"]\n",
    "\n",
    "        print(f\"\\nInitialization complete! {result['chunks_count']} chunks indexed.\")\n",
    "\n",
    "    def chat(self, query: str, user_id: Optional[str] = None) -> dict:\n",
    "        \"\"\"Process a chat message and return the response.\n",
    "\n",
    "        User and session context is attached to all auto-traced spans.\n",
    "        \"\"\"\n",
    "        # Set session and user context for all traces\n",
    "        Netra.set_session_id(self.session_id)\n",
    "        if user_id:\n",
    "            Netra.set_user_id(user_id)\n",
    "\n",
    "        # Retrieve relevant chunks (auto-traced)\n",
    "        retrieved_chunks = retrieve_chunks(query, self.collection)\n",
    "\n",
    "        # Generate answer (auto-traced)\n",
    "        result = generate_answer(query, retrieved_chunks)\n",
    "\n",
    "        # Update conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": result[\"answer\"]})\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"token_usage\": result[\"token_usage\"],\n",
    "            \"session_id\": self.session_id\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"PDFChatbot class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Create a Sample PDF\n",
    "\n",
    "Upload your own PDF or create a sample one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload your own PDF (uncomment to use)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# pdf_path = list(uploaded.keys())[0]\n",
    "\n",
    "# Option 2: Create a sample PDF for testing\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_sample_pdf(filename: str = \"sample_document.pdf\"):\n",
    "    \"\"\"Create a sample PDF for testing.\"\"\"\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Page 1: Introduction\n",
    "    c.setFont(\"Helvetica-Bold\", 24)\n",
    "    c.drawString(100, height - 100, \"Introduction to Machine Learning\")\n",
    "\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    text = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that enables systems to learn\n",
    "    and improve from experience without being explicitly programmed. The term was coined\n",
    "    by Arthur Samuel in 1959 while at IBM.\n",
    "\n",
    "    Machine learning algorithms build a mathematical model based on sample data, known as\n",
    "    training data, in order to make predictions or decisions without being explicitly\n",
    "    programmed to perform the task.\n",
    "\n",
    "    The primary aim is to allow computers to learn automatically without human intervention\n",
    "    or assistance and adjust actions accordingly.\n",
    "    \"\"\"\n",
    "\n",
    "    y = height - 150\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        c.drawString(100, y, line.strip())\n",
    "        y -= 20\n",
    "\n",
    "    # Page 2: Types of ML\n",
    "    c.showPage()\n",
    "    c.setFont(\"Helvetica-Bold\", 18)\n",
    "    c.drawString(100, height - 100, \"Types of Machine Learning\")\n",
    "\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    text = \"\"\"\n",
    "    There are three main types of machine learning:\n",
    "\n",
    "    1. Supervised Learning: The algorithm learns from labeled training data and makes\n",
    "       predictions based on that data. Examples include classification and regression.\n",
    "\n",
    "    2. Unsupervised Learning: The algorithm learns patterns from unlabeled data without\n",
    "       any guidance. Examples include clustering and dimensionality reduction.\n",
    "\n",
    "    3. Reinforcement Learning: The algorithm learns through interaction with an environment,\n",
    "       receiving rewards or penalties for actions. Used in robotics and game playing.\n",
    "\n",
    "    Each type has its own applications and is suited for different kinds of problems.\n",
    "    \"\"\"\n",
    "\n",
    "    y = height - 150\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        c.drawString(100, y, line.strip())\n",
    "        y -= 20\n",
    "\n",
    "    # Page 3: Applications\n",
    "    c.showPage()\n",
    "    c.setFont(\"Helvetica-Bold\", 18)\n",
    "    c.drawString(100, height - 100, \"Applications of Machine Learning\")\n",
    "\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    text = \"\"\"\n",
    "    Machine learning has numerous real-world applications:\n",
    "\n",
    "    - Image Recognition: Identifying objects, faces, and scenes in images\n",
    "    - Natural Language Processing: Translation, sentiment analysis, chatbots\n",
    "    - Recommendation Systems: Netflix, Amazon, Spotify recommendations\n",
    "    - Autonomous Vehicles: Self-driving cars use ML for navigation\n",
    "    - Healthcare: Disease diagnosis, drug discovery, personalized treatment\n",
    "    - Financial Services: Fraud detection, algorithmic trading, credit scoring\n",
    "\n",
    "    The field continues to grow rapidly with new applications emerging regularly.\n",
    "    \"\"\"\n",
    "\n",
    "    y = height - 150\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        c.drawString(100, y, line.strip())\n",
    "        y -= 20\n",
    "\n",
    "    c.save()\n",
    "    print(f\"Created sample PDF: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Create the sample PDF\n",
    "pdf_path = create_sample_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 8: Initialize and Test the Chatbot\n",
    "\n",
    "Now let's create and test our auto-traced PDF chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chatbot\n",
    "chatbot = PDFChatbot(pdf_path)\n",
    "chatbot.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single question\n",
    "response = chatbot.chat(\n",
    "    \"What are the key findings?\",\n",
    "    user_id=\"user-123\"\n",
    ")\n",
    "print(f\"Answer: {response['answer']}\")\n",
    "print(f\"\\nTokens used: {response['token_usage']['total']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple questions\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"Who coined the term machine learning and when?\",\n",
    "    \"What are some applications of machine learning?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing the RAG Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n--- Question {i} ---\")\n",
    "    print(f\"Q: {question}\")\n",
    "\n",
    "    response = chatbot.chat(question, user_id=\"test-user\")\n",
    "\n",
    "    print(f\"\\nA: {response['answer']}\")\n",
    "    print(f\"\\n[Tokens: {response['token_usage']['total']}]\")\n",
    "    print(f\"[Retrieved {len(response['retrieved_chunks'])} chunks | \"\n",
    "          f\"Max similarity: {max(c['similarity_score'] for c in response['retrieved_chunks']):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 9 (Optional): Using Decorators for Custom Spans\n",
    "\n",
    "Auto-instrumentation handles most cases, but you can add structure with decorators when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netra.decorators import workflow, task, span\n",
    "\n",
    "@task(name=\"load-pdf\")\n",
    "def load_pdf_traced(file_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file with tracing.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "@task(name=\"chunk-text\")\n",
    "def chunk_text_traced(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks with tracing.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "class PDFChatbotWithDecorators:\n",
    "    \"\"\"RAG Pipeline with decorator-based tracing.\"\"\"\n",
    "\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.collection = None\n",
    "        self.chunks: List[str] = []\n",
    "        self.conversation_history: List[Dict] = []\n",
    "\n",
    "    @task(name=\"document-ingestion\")\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the vector store with PDF content.\"\"\"\n",
    "        pdf_text = load_pdf_traced(self.pdf_path)\n",
    "        self.chunks = chunk_text_traced(pdf_text)\n",
    "        embeddings = generate_embeddings(self.chunks)\n",
    "\n",
    "        self.collection = chroma_client.create_collection(name=f\"pdf_{self.session_id[:8]}\")\n",
    "        self.collection.add(\n",
    "            documents=self.chunks,\n",
    "            embeddings=embeddings,\n",
    "            ids=[f\"chunk_{i}\" for i in range(len(self.chunks))]\n",
    "        )\n",
    "        print(f\"Initialized with {len(self.chunks)} chunks\")\n",
    "\n",
    "    @workflow(name=\"pdf-qa-query\")\n",
    "    def chat(self, query: str, user_id: Optional[str] = None) -> Dict:\n",
    "        \"\"\"Process a chat message within a workflow span.\"\"\"\n",
    "        Netra.set_session_id(self.session_id)\n",
    "        if user_id:\n",
    "            Netra.set_user_id(user_id)\n",
    "\n",
    "        retrieved = self._retrieve(query)\n",
    "        result = generate_answer(query, retrieved)\n",
    "\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": result[\"answer\"]})\n",
    "\n",
    "        return {\"query\": query, \"answer\": result[\"answer\"], \"retrieved_chunks\": retrieved}\n",
    "\n",
    "    @task(name=\"retrieval\")\n",
    "    def _retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant chunks within a task span.\"\"\"\n",
    "        return retrieve_chunks(query, self.collection, top_k)\n",
    "\n",
    "\n",
    "print(\"Decorator-based chatbot defined!\")\n",
    "print(\"\")\n",
    "print(\"Decorator reference:\")\n",
    "print(\"  @workflow - Top-level pipeline or request handler\")\n",
    "print(\"  @task     - Discrete unit of work within a workflow\")\n",
    "print(\"  @span     - Fine-grained tracing for specific operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What You'll See in the Dashboard\n",
    "\n",
    "After running this notebook, check the Netra dashboard for:\n",
    "\n",
    "- **OpenAI spans** showing model, tokens, cost, and full prompt/response\n",
    "- **ChromaDB spans** showing query timing and results\n",
    "- **User and session IDs** attached to all spans for filtering\n",
    "\n",
    "## Documentation Links\n",
    "\n",
    "- [Netra Documentation](https://docs.getnetra.ai)\n",
    "- [Auto-Instrumentation Guide](https://docs.getnetra.ai/Observability/Traces/auto-instrumentation)\n",
    "- [Decorators Guide](https://docs.getnetra.ai/Observability/Traces/decorators)\n",
    "- [Traces API](https://docs.getnetra.ai/Observability/Traces)\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Evaluating RAG Quality](/Cookbooks/evaluation/evaluating-rag-quality) - Add quality metrics and test suites\n",
    "- [Tracing LangChain Agents](/Cookbooks/observability/tracing-langchain-agents) - Apply similar tracing to LangChain\n",
    "- [Tracing CrewAI Pipelines](/Cookbooks/observability/tracing-crewai-pipelines) - Trace multi-agent workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
