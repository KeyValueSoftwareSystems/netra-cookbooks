{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluating Agent Decisions\n",
    "\n",
    "This notebook shows you how to systematically evaluate AI agent behavior—measuring tool selection accuracy, detecting escalation errors, and tracking workflow completion. Unlike simple LLM evaluation, agent evaluation addresses multi-step decision chains where errors compound.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Build test datasets that properly evaluate agent behavior\n",
    "- Create evaluators for tool selection accuracy\n",
    "- Measure escalation accuracy and over/under-escalation\n",
    "- Track multi-step workflow completion\n",
    "- Debug agent failures using traces\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python >=3.10, <3.14\n",
    "- OpenAI API key\n",
    "- Netra API key ([Get started here](https://docs.getnetra.ai/quick-start/Overview))\n",
    "- A LangChain or similar agent framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 0: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install netra-sdk openai langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"NETRA_API_KEY\"] = getpass(\"Enter your Netra API Key:\")\n",
    "os.environ[\"NETRA_OTLP_ENDPOINT\"] = getpass(\"Enter your Netra OTLP Endpoint:\")\n",
    "\n",
    "print(\"API keys configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Netra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netra import Netra\n",
    "from netra.instrumentation.instruments import InstrumentSet\n",
    "\n",
    "Netra.init(\n",
    "    app_name=\"agent-evaluation\",\n",
    "    headers=f\"x-api-key={os.getenv('NETRA_API_KEY')}\",\n",
    "    environment=\"testing\",\n",
    "    trace_content=True,\n",
    "    instruments={InstrumentSet.OPENAI, InstrumentSet.LANGCHAIN},\n",
    ")\n",
    "\n",
    "print(\"Netra initialized for agent evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Create Test Dataset Structure\n",
    "\n",
    "Define test cases with expected agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "@dataclass\n",
    "class AgentTestCase:\n",
    "    \"\"\"Test case for evaluating agent decisions.\"\"\"\n",
    "    test_id: str\n",
    "    query: str\n",
    "    category: str  # e.g., \"single_tool\", \"multi_tool\", \"escalation\"\n",
    "    expected_tools: List[str]  # Tools the agent should use\n",
    "    forbidden_tools: List[str] = None  # Tools the agent should NOT use\n",
    "    should_escalate: bool = False  # Whether escalation is appropriate\n",
    "    expected_outcome: str = \"success\"  # \"success\", \"failure\", \"escalation\"\n",
    "\n",
    "# Define test dataset\n",
    "TEST_DATASET = [\n",
    "    AgentTestCase(\n",
    "        test_id=\"single-tool-1\",\n",
    "        query=\"What is the return policy?\",\n",
    "        category=\"single_tool\",\n",
    "        expected_tools=[\"search_kb\"],\n",
    "        forbidden_tools=[\"check_order_status\", \"escalate_to_human\"],\n",
    "        should_escalate=False,\n",
    "        expected_outcome=\"success\"\n",
    "    ),\n",
    "    AgentTestCase(\n",
    "        test_id=\"multi-tool-1\",\n",
    "        query=\"I have ticket TKT-001 about a damaged item. Can you check the order?\",\n",
    "        category=\"multi_tool\",\n",
    "        expected_tools=[\"lookup_ticket\", \"check_order_status\"],\n",
    "        forbidden_tools=[\"escalate_to_human\"],\n",
    "        should_escalate=False,\n",
    "        expected_outcome=\"success\"\n",
    "    ),\n",
    "    AgentTestCase(\n",
    "        test_id=\"escalation-1\",\n",
    "        query=\"I'm FURIOUS! Your product destroyed my business! I need to speak to someone NOW!\",\n",
    "        category=\"escalation\",\n",
    "        expected_tools=[\"escalate_to_human\"],\n",
    "        forbidden_tools=[],\n",
    "        should_escalate=True,\n",
    "        expected_outcome=\"escalation\"\n",
    "    ),\n",
    "    AgentTestCase(\n",
    "        test_id=\"no-tool-1\",\n",
    "        query=\"Thank you for your help!\",\n",
    "        category=\"no_tool\",\n",
    "        expected_tools=[],\n",
    "        forbidden_tools=[\"lookup_ticket\", \"check_order_status\", \"escalate_to_human\"],\n",
    "        should_escalate=False,\n",
    "        expected_outcome=\"success\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Test dataset created with {len(TEST_DATASET)} cases\")\n",
    "for case in TEST_DATASET:\n",
    "    print(f\"  - {case.test_id} ({case.category}): {case.query[:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Implement Agent Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class AgentEvaluator:\n",
    "    \"\"\"Evaluate agent behavior against expected decisions.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_tool_selection(\n",
    "        expected_tools: List[str],\n",
    "        actual_tools: List[str],\n",
    "        forbidden_tools: List[str] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate whether agent selected correct tools.\"\"\"\n",
    "        if forbidden_tools is None:\n",
    "            forbidden_tools = []\n",
    "\n",
    "        # Check for correct tool usage\n",
    "        correct_tools = set(expected_tools) & set(actual_tools)\n",
    "        missing_tools = set(expected_tools) - set(actual_tools)\n",
    "        extra_tools = set(actual_tools) - set(expected_tools)\n",
    "        forbidden_used = set(forbidden_tools) & set(actual_tools)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        if not expected_tools:  # No tools expected\n",
    "            accuracy = 0 if actual_tools else 1\n",
    "        else:\n",
    "            accuracy = len(correct_tools) / len(expected_tools)\n",
    "\n",
    "        # Penalize forbidden tool usage\n",
    "        if forbidden_used:\n",
    "            accuracy *= 0.5\n",
    "\n",
    "        return {\n",
    "            \"passed\": len(missing_tools) == 0 and len(forbidden_used) == 0,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_tools\": list(correct_tools),\n",
    "            \"missing_tools\": list(missing_tools),\n",
    "            \"extra_tools\": list(extra_tools),\n",
    "            \"forbidden_used\": list(forbidden_used),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_escalation(\n",
    "        should_escalate: bool,\n",
    "        did_escalate: bool\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate escalation decision accuracy.\"\"\"\n",
    "        if should_escalate == did_escalate:\n",
    "            return {\"passed\": True, \"type\": \"correct\", \"score\": 1.0}\n",
    "        elif should_escalate and not did_escalate:\n",
    "            # Under-escalation: worse than over-escalation\n",
    "            return {\"passed\": False, \"type\": \"under_escalation\", \"score\": 0.2}\n",
    "        else:\n",
    "            # Over-escalation: less critical but still wrong\n",
    "            return {\"passed\": False, \"type\": \"over_escalation\", \"score\": 0.5}\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_workflow(\n",
    "        tool_sequence: List[str],\n",
    "        expected_first_tool: str\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate multi-step workflow execution.\"\"\"\n",
    "        if not tool_sequence:\n",
    "            return {\"passed\": False, \"sequence_valid\": False, \"score\": 0.0}\n",
    "\n",
    "        # Check if workflow starts with expected tool\n",
    "        first_correct = tool_sequence[0] == expected_first_tool\n",
    "        sequence_valid = len(tool_sequence) > 1  # Multi-step workflow\n",
    "\n",
    "        return {\n",
    "            \"passed\": first_correct and sequence_valid,\n",
    "            \"first_tool_correct\": first_correct,\n",
    "            \"sequence_valid\": sequence_valid,\n",
    "            \"tool_count\": len(tool_sequence),\n",
    "            \"score\": 1.0 if (first_correct and sequence_valid) else 0.5 if first_correct else 0.0,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Agent evaluators implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Simulate Agent Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAgent:\n",
    "    \"\"\"Simulate agent behavior for testing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tool_calls = []\n",
    "        self.escalated = False\n",
    "\n",
    "    def process_query(self, test_case: AgentTestCase) -> Dict:\n",
    "        \"\"\"Simulate agent processing a query.\"\"\"\n",
    "        # Simulated tool usage based on query analysis\n",
    "        if test_case.test_id == \"single-tool-1\":\n",
    "            self.tool_calls = [\"search_kb\"]\n",
    "            self.escalated = False\n",
    "        elif test_case.test_id == \"multi-tool-1\":\n",
    "            self.tool_calls = [\"lookup_ticket\", \"check_order_status\"]\n",
    "            self.escalated = False\n",
    "        elif test_case.test_id == \"escalation-1\":\n",
    "            self.tool_calls = []\n",
    "            self.escalated = True\n",
    "        elif test_case.test_id == \"no-tool-1\":\n",
    "            self.tool_calls = []\n",
    "            self.escalated = False\n",
    "        else:\n",
    "            # Random behavior for unknown tests\n",
    "            self.tool_calls = []\n",
    "            self.escalated = False\n",
    "\n",
    "        return {\n",
    "            \"test_id\": test_case.test_id,\n",
    "            \"tool_calls\": self.tool_calls,\n",
    "            \"escalated\": self.escalated\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Simulated agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Run Agent Evaluation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent and evaluator\n",
    "agent = SimulatedAgent()\n",
    "results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Running Agent Evaluation Tests\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for test_case in TEST_DATASET:\n",
    "    print(f\"\\n[{test_case.test_id}] {test_case.query}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Run agent\n",
    "    agent_result = agent.process_query(test_case)\n",
    "\n",
    "    # Evaluate tool selection\n",
    "    tool_eval = AgentEvaluator.evaluate_tool_selection(\n",
    "        expected_tools=test_case.expected_tools,\n",
    "        actual_tools=agent_result[\"tool_calls\"],\n",
    "        forbidden_tools=test_case.forbidden_tools\n",
    "    )\n",
    "\n",
    "    # Evaluate escalation\n",
    "    escalation_eval = AgentEvaluator.evaluate_escalation(\n",
    "        should_escalate=test_case.should_escalate,\n",
    "        did_escalate=agent_result[\"escalated\"]\n",
    "    )\n",
    "\n",
    "    # Overall test result\n",
    "    test_passed = tool_eval[\"passed\"] and escalation_eval[\"passed\"]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Category: {test_case.category}\")\n",
    "    print(f\"Expected Tools: {test_case.expected_tools}\")\n",
    "    print(f\"Actual Tools: {agent_result['tool_calls']}\")\n",
    "    print(f\"\\nTool Selection:\")\n",
    "    print(f\"  Correct: {tool_eval['correct_tools']}\")\n",
    "    print(f\"  Missing: {tool_eval['missing_tools']}\")\n",
    "    print(f\"  Accuracy: {tool_eval['accuracy']:.1%}\")\n",
    "    print(f\"\\nEscalation:\")\n",
    "    print(f\"  Should Escalate: {test_case.should_escalate}\")\n",
    "    print(f\"  Did Escalate: {agent_result['escalated']}\")\n",
    "    print(f\"  Type: {escalation_eval['type']}\")\n",
    "    print(f\"\\nResult: {'✓ PASS' if test_passed else '✗ FAIL'}\")\n",
    "\n",
    "    # Store result\n",
    "    results.append({\n",
    "        \"test_id\": test_case.test_id,\n",
    "        \"category\": test_case.category,\n",
    "        \"passed\": test_passed,\n",
    "        \"tool_eval\": tool_eval,\n",
    "        \"escalation_eval\": escalation_eval,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Test Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_tests = len(results)\n",
    "passed_tests = sum(1 for r in results if r[\"passed\"])\n",
    "pass_rate = passed_tests / total_tests if total_tests > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal Tests: {total_tests}\")\n",
    "print(f\"Passed: {passed_tests}\")\n",
    "print(f\"Failed: {total_tests - passed_tests}\")\n",
    "print(f\"Pass Rate: {pass_rate:.1%}\")\n",
    "\n",
    "# Breakdown by category\n",
    "categories = {}\n",
    "for result in results:\n",
    "    cat = result[\"category\"]\n",
    "    if cat not in categories:\n",
    "        categories[cat] = {\"total\": 0, \"passed\": 0}\n",
    "    categories[cat][\"total\"] += 1\n",
    "    if result[\"passed\"]:\n",
    "        categories[cat][\"passed\"] += 1\n",
    "\n",
    "print(\"\\nBy Category:\")\n",
    "for cat, stats in sorted(categories.items()):\n",
    "    rate = stats[\"passed\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0\n",
    "    print(f\"  {cat}: {stats['passed']}/{stats['total']} ({rate:.1%})\")\n",
    "\n",
    "# Failure analysis\n",
    "failures = [r for r in results if not r[\"passed\"]]\n",
    "if failures:\n",
    "    print(f\"\\nFailed Tests ({len(failures)}):\")\n",
    "    for failure in failures:\n",
    "        print(f\"\\n  {failure['test_id']}:\")\n",
    "        if not failure[\"tool_eval\"][\"passed\"]:\n",
    "            print(f\"    Tool Selection Issue:\")\n",
    "            if failure[\"tool_eval\"][\"missing_tools\"]:\n",
    "                print(f\"      Missing: {failure['tool_eval']['missing_tools']}\")\n",
    "            if failure[\"tool_eval\"][\"forbidden_used\"]:\n",
    "                print(f\"      Forbidden: {failure['tool_eval']['forbidden_used']}\")\n",
    "        if not failure[\"escalation_eval\"][\"passed\"]:\n",
    "            print(f\"    Escalation Issue: {failure['escalation_eval']['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 8: Debugging with Traces\n",
    "\n",
    "Use Netra traces to debug failed tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "DEBUGGING WITH TRACES\n",
    "=\"*70)\n",
    "\n",
    "When an agent test fails, use Netra traces to investigate:\n",
    "\n",
    "1. **Tool Call Sequence**\n",
    "   - Check the order of tool invocations\n",
    "   - Verify each tool's inputs and outputs\n",
    "   - Look for unexpected tool calls\n",
    "\n",
    "2. **LLM Reasoning**\n",
    "   - Review the exact prompt sent to the LLM\n",
    "   - Check the model's reasoning in the completion\n",
    "   - Look for prompt ambiguity or missing instructions\n",
    "\n",
    "3. **Tool Definitions**\n",
    "   - Verify tool descriptions are clear and distinct\n",
    "   - Check if similar tools might confuse the agent\n",
    "   - Review tool parameters and expected outputs\n",
    "\n",
    "4. **Context and History**\n",
    "   - Check if conversation history affects decisions\n",
    "   - Verify session/user context is properly set\n",
    "   - Look for carryover from previous queries\n",
    "\n",
    "5. **Common Failure Patterns**\n",
    "   - Wrong tool selection → Improve tool descriptions\n",
    "   - Infinite loops → Adjust loop detection in agent\n",
    "   - Missing escalation → Refine escalation criteria\n",
    "   - Extra tool calls → Add tool usage constraints\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Step 9: Improvement Recommendations\n",
    "\n",
    "Based on evaluation results, implement improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pass_rate < 1.0:\n",
    "    print(\"\"\"\n",
    "IMPROVEMENT RECOMMENDATIONS\n",
    "=\"*70)\n",
    "\n",
    "Based on your agent evaluation results, consider these improvements:\n",
    "\n",
    "1. **Refine Tool Descriptions**\n",
    "   - Make tool descriptions more distinct and specific\n",
    "   - Include examples of when each tool should be used\n",
    "   - Add notes about when NOT to use a tool\n",
    "\n",
    "2. **Improve System Prompts**\n",
    "   - Add explicit instructions for escalation criteria\n",
    "   - Include decision trees for complex scenarios\n",
    "   - Specify tool usage order for multi-step workflows\n",
    "\n",
    "3. **Add Tool Constraints**\n",
    "   - Limit tool call sequences to prevent loops\n",
    "   - Require specific tools before others (dependencies)\n",
    "   - Set maximum tool calls per query\n",
    "\n",
    "4. **Implement Pre/Post-Checks**\n",
    "   - Validate tool selection before execution\n",
    "   - Check escalation appropriateness before routing\n",
    "   - Verify workflow completion before returning\n",
    "\n",
    "5. **Run Continuous Evaluation**\n",
    "   - Weekly regression tests to detect degradation\n",
    "   - Evaluate after prompt changes\n",
    "   - Test after adding new tools\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\\n✓ All tests passed! Continue monitoring with regular evaluation runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Metrics for Agent Evaluation\n",
    "\n",
    "| Metric | Good | Warning | Action Needed |\n",
    "|--------|------|---------|---------------|\n",
    "| Tool Accuracy | 95-100% | 80-95% | <80% |\n",
    "| Escalation Accuracy | 100% | 90-100% | <90% |\n",
    "| Workflow Completion | 95-100% | 80-95% | <80% |\n",
    "| Multi-step Success | 90-100% | 70-90% | <70% |\n",
    "\n",
    "## Documentation Links\n",
    "\n",
    "- [Netra Documentation](https://docs.getnetra.ai)\n",
    "- [Agent Evaluation](https://docs.getnetra.ai/Evaluation/Agent-Evaluation)\n",
    "- [Traces for Debugging](https://docs.getnetra.ai/Observability/Traces/Debugging)\n",
    "- [Evaluation Framework](https://docs.getnetra.ai/Evaluation)\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Tracing LangChain Agents](/Cookbooks/observability/tracing-langchain-agents) - Add observability to agents\n",
    "- [Custom Evaluator Patterns](/Cookbooks/evaluation/custom-evaluator-patterns) - Build custom evaluators\n",
    "- [A/B Testing Configurations](/Cookbooks/evaluation/ab-testing-configurations) - Compare agent configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
