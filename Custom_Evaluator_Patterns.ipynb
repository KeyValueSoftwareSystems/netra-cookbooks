{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Custom Evaluator Patterns\n",
    "\n",
    "This notebook provides practical patterns for building domain-specific evaluators to measure output quality consistently. Learn to combine LLM-as-Judge and code evaluators for comprehensive quality measurement.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Build LLM-as-Judge evaluators for subjective quality assessment\n",
    "- Create code evaluators for deterministic checks\n",
    "- Implement multi-stage evaluation pipelines\n",
    "- Combine multiple evaluators with composite scoring\n",
    "- Apply evaluators to real-world content generation tasks\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python >=3.10, <3.14\n",
    "- OpenAI API key\n",
    "- Netra API key ([Get started here](https://docs.getnetra.ai/quick-start/Overview))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 0: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install netra-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"NETRA_API_KEY\"] = getpass(\"Enter your Netra API Key:\")\n",
    "os.environ[\"NETRA_OTLP_ENDPOINT\"] = getpass(\"Enter your Netra OTLP Endpoint:\")\n",
    "\n",
    "print(\"API keys configured!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Netra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netra import Netra\n",
    "from netra.instrumentation.instruments import InstrumentSet\n",
    "\n",
    "Netra.init(\n",
    "    app_name=\"custom-evaluators\",\n",
    "    headers=f\"x-api-key={os.getenv('NETRA_API_KEY')}\",\n",
    "    environment=\"testing\",\n",
    "    trace_content=True,\n",
    "    instruments={InstrumentSet.OPENAI},\n",
    ")\n",
    "\n",
    "print(\"Netra initialized for custom evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Implement LLM-as-Judge Evaluators\n",
    "\n",
    "Create evaluators that use LLMs to assess subjective quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Dict, List\n",
    "\n",
    "class LLMJudgeEvaluator:\n",
    "    \"\"\"LLM-as-Judge evaluator for subjective quality assessment.\"\"\"\n",
    "\n",
    "    def __init__(self, criteria: Dict[str, str], weights: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with criteria.\n",
    "        \n",
    "        Args:\n",
    "            criteria: Dict of criterion_name -> description\n",
    "            weights: Optional dict of criterion_name -> weight (default: equal)\n",
    "        \"\"\"\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.criteria = criteria\n",
    "        \n",
    "        if weights is None:\n",
    "            # Equal weighting\n",
    "            weight = 1.0 / len(criteria)\n",
    "            self.weights = {k: weight for k in criteria}\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "    def evaluate(self, output: str, context: str = \"\") -> Dict:\n",
    "        \"\"\"Evaluate output on all criteria.\"\"\"\n",
    "        criteria_text = \"\\n\".join(\n",
    "            f\"- {name}: {desc}\" for name, desc in self.criteria.items()\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert evaluator. Rate the following output on these criteria:\n",
    "\n",
    "{criteria_text}\n",
    "\n",
    "For each criterion, provide a score from 1-10 and brief explanation.\n",
    "Format: criterion_name: score (explanation)\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Output to evaluate:\n",
    "{output}\n",
    "\n",
    "Evaluations:\"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        scores = self._parse_scores(response.choices[0].message.content)\n",
    "        \n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"weighted_score\": self._calculate_weighted_score(scores),\n",
    "            \"feedback\": response.choices[0].message.content\n",
    "        }\n",
    "\n",
    "    def _parse_scores(self, feedback: str) -> Dict[str, float]:\n",
    "        \"\"\"Parse scores from evaluator feedback.\"\"\"\n",
    "        scores = {}\n",
    "        for line in feedback.split(\"\\n\"):\n",
    "            for criterion in self.criteria:\n",
    "                if criterion.lower() in line.lower():\n",
    "                    # Extract first number found\n",
    "                    import re\n",
    "                    matches = re.findall(r\"\\d+\", line)\n",
    "                    if matches:\n",
    "                        score = min(int(matches[0]), 10)\n",
    "                        scores[criterion] = float(score)\n",
    "        \n",
    "        # Fill missing scores with average\n",
    "        if scores:\n",
    "            avg = sum(scores.values()) / len(scores)\n",
    "        else:\n",
    "            avg = 5.0\n",
    "        \n",
    "        for criterion in self.criteria:\n",
    "            if criterion not in scores:\n",
    "                scores[criterion] = avg\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def _calculate_weighted_score(self, scores: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate weighted average score.\"\"\"\n",
    "        total = sum(\n",
    "            scores.get(criterion, 5.0) * self.weights.get(criterion, 0)\n",
    "            for criterion in self.criteria\n",
    "        )\n",
    "        return min(max(total, 1), 10)\n",
    "\n",
    "\n",
    "print(\"LLM-as-Judge evaluator implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Implement Code Evaluators\n",
    "\n",
    "Create deterministic evaluators for structural and format checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class CodeEvaluator:\n",
    "    \"\"\"Deterministic code-based evaluator for format and structure checks.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def check_length(output: str, min_length: int = 0, max_length: int = None) -> Dict:\n",
    "        \"\"\"Check if output length is within acceptable range.\"\"\"\n",
    "        length = len(output)\n",
    "        \n",
    "        passed = length >= min_length\n",
    "        if max_length:\n",
    "            passed = passed and length <= max_length\n",
    "        \n",
    "        return {\n",
    "            \"passed\": passed,\n",
    "            \"score\": 10.0 if passed else 3.0,\n",
    "            \"actual_length\": length,\n",
    "            \"min_length\": min_length,\n",
    "            \"max_length\": max_length\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def check_json_valid(output: str) -> Dict:\n",
    "        \"\"\"Check if output is valid JSON.\"\"\"\n",
    "        try:\n",
    "            json.loads(output)\n",
    "            return {\"passed\": True, \"score\": 10.0, \"error\": None}\n",
    "        except json.JSONDecodeError as e:\n",
    "            return {\"passed\": False, \"score\": 0.0, \"error\": str(e)}\n",
    "\n",
    "    @staticmethod\n",
    "    def check_required_fields(output: str, required_fields: List[str]) -> Dict:\n",
    "        \"\"\"Check if required fields/keywords are present.\"\"\"\n",
    "        found_fields = [f for f in required_fields if f.lower() in output.lower()]\n",
    "        coverage = len(found_fields) / len(required_fields) if required_fields else 1.0\n",
    "        \n",
    "        return {\n",
    "            \"passed\": coverage == 1.0,\n",
    "            \"score\": coverage * 10.0,\n",
    "            \"found_fields\": found_fields,\n",
    "            \"missing_fields\": [f for f in required_fields if f not in found_fields]\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def check_word_count(output: str, min_words: int = 0, max_words: int = None) -> Dict:\n",
    "        \"\"\"Check if word count is within range.\"\"\"\n",
    "        word_count = len(output.split())\n",
    "        passed = word_count >= min_words\n",
    "        \n",
    "        if max_words:\n",
    "            passed = passed and word_count <= max_words\n",
    "        \n",
    "        return {\n",
    "            \"passed\": passed,\n",
    "            \"score\": 10.0 if passed else 3.0,\n",
    "            \"actual_words\": word_count,\n",
    "            \"min_words\": min_words,\n",
    "            \"max_words\": max_words\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def check_no_forbidden_content(output: str, forbidden: List[str]) -> Dict:\n",
    "        \"\"\"Check that forbidden words/phrases are not present.\"\"\"\n",
    "        found_forbidden = [f for f in forbidden if f.lower() in output.lower()]\n",
    "        passed = len(found_forbidden) == 0\n",
    "        \n",
    "        return {\n",
    "            \"passed\": passed,\n",
    "            \"score\": 10.0 if passed else 0.0,\n",
    "            \"found_forbidden\": found_forbidden\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Code evaluators implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Define Multi-Stage Evaluation Pipeline\n",
    "\n",
    "Combine multiple evaluators for comprehensive quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeEvaluator:\n",
    "    \"\"\"Combine multiple evaluators for comprehensive assessment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Content quality evaluator\n",
    "        self.content_judge = LLMJudgeEvaluator(\n",
    "            criteria={\n",
    "                \"clarity\": \"Is the text clear and easy to understand?\",\n",
    "                \"completeness\": \"Does the output address all aspects of the request?\",\n",
    "                \"accuracy\": \"Is the content factually accurate?\",\n",
    "                \"engagement\": \"Is the content interesting and well-written?\"\n",
    "            },\n",
    "            weights={\n",
    "                \"clarity\": 0.25,\n",
    "                \"completeness\": 0.25,\n",
    "                \"accuracy\": 0.25,\n",
    "                \"engagement\": 0.25\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def evaluate_blog_post(self, content: str) -> Dict:\n",
    "        \"\"\"Evaluate a blog post with multiple criteria.\"\"\"\n",
    "        results = {\"evaluations\": {}}\n",
    "        \n",
    "        # LLM-as-Judge evaluation\n",
    "        judge_result = self.content_judge.evaluate(content)\n",
    "        results[\"evaluations\"][\"content_quality\"] = judge_result\n",
    "        \n",
    "        # Code-based evaluations\n",
    "        results[\"evaluations\"][\"length_check\"] = CodeEvaluator.check_word_count(\n",
    "            content, min_words=100, max_words=2000\n",
    "        )\n",
    "        \n",
    "        results[\"evaluations\"][\"structure_check\"] = CodeEvaluator.check_required_fields(\n",
    "            content, required_fields=[\"introduction\", \"conclusion\"]\n",
    "        )\n",
    "        \n",
    "        results[\"evaluations\"][\"forbidden_content\"] = CodeEvaluator.check_no_forbidden_content(\n",
    "            content, forbidden=[\"TODO\", \"[EDIT]\", \"[CITATION NEEDED]\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate composite score\n",
    "        scores = []\n",
    "        for eval_result in results[\"evaluations\"].values():\n",
    "            if isinstance(eval_result, dict) and \"score\" in eval_result:\n",
    "                scores.append(eval_result[\"score\"])\n",
    "            elif isinstance(eval_result, dict) and \"weighted_score\" in eval_result:\n",
    "                scores.append(eval_result[\"weighted_score\"])\n",
    "        \n",
    "        results[\"composite_score\"] = sum(scores) / len(scores) if scores else 5.0\n",
    "        results[\"passed\"] = results[\"composite_score\"] >= 7.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def evaluate_email(self, content: str) -> Dict:\n",
    "        \"\"\"Evaluate an email with format and tone checks.\"\"\"\n",
    "        results = {\"evaluations\": {}}\n",
    "        \n",
    "        # Tone evaluator\n",
    "        tone_judge = LLMJudgeEvaluator(\n",
    "            criteria={\n",
    "                \"professionalism\": \"Is the tone professional and appropriate?\",\n",
    "                \"conciseness\": \"Is the email concise and to the point?\",\n",
    "                \"clarity\": \"Is the action or request clear?\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        results[\"evaluations\"][\"tone\"] = tone_judge.evaluate(content)\n",
    "        \n",
    "        # Format checks\n",
    "        results[\"evaluations\"][\"length\"] = CodeEvaluator.check_word_count(\n",
    "            content, min_words=20, max_words=500\n",
    "        )\n",
    "        \n",
    "        results[\"evaluations\"][\"structure\"] = CodeEvaluator.check_required_fields(\n",
    "            content, required_fields=[\"hello\", \"regards\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate composite score\n",
    "        scores = []\n",
    "        for eval_result in results[\"evaluations\"].values():\n",
    "            if isinstance(eval_result, dict) and \"score\" in eval_result:\n",
    "                scores.append(eval_result[\"score\"])\n",
    "            elif isinstance(eval_result, dict) and \"weighted_score\" in eval_result:\n",
    "                scores.append(eval_result[\"weighted_score\"])\n",
    "        \n",
    "        results[\"composite_score\"] = sum(scores) / len(scores) if scores else 5.0\n",
    "        results[\"passed\"] = results[\"composite_score\"] >= 7.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"Composite evaluator implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Test Evaluators on Content\n",
    "\n",
    "Apply evaluators to real-world content examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = CompositeEvaluator()\n",
    "\n",
    "# Test 1: Blog post\n",
    "blog_post = \"\"\"\n",
    "Introduction\n",
    "\n",
    "Artificial intelligence is transforming how we work. From automation to decision support,\n",
    "AI tools are becoming essential across industries. Understanding AI fundamentals is now\n",
    "a core skill for modern professionals.\n",
    "\n",
    "What is AI?\n",
    "\n",
    "Artificial Intelligence refers to computer systems designed to perform tasks that typically\n",
    "require human intelligence. These include learning from data, recognizing patterns, and\n",
    "making decisions.\n",
    "\n",
    "Key Benefits\n",
    "\n",
    "AI systems can process vast amounts of data quickly, identify subtle patterns, and operate\n",
    "24/7 without fatigue. Organizations using AI report improved efficiency and better insights.\n",
    "\n",
    "Implementation Challenges\n",
    "\n",
    "While powerful, AI implementation requires careful planning. Data quality, model training,\n",
    "and ongoing monitoring are critical success factors.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "AI is no longer optional in modern business. Organizations that embrace AI thoughtfully\n",
    "will gain competitive advantages in their markets.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING BLOG POST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "blog_result = evaluator.evaluate_blog_post(blog_post)\n",
    "\n",
    "print(f\"\\nComposite Score: {blog_result['composite_score']:.1f}/10\")\n",
    "print(f\"Status: {'✓ PASS' if blog_result['passed'] else '✗ FAIL'}\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for eval_name, result in blog_result[\"evaluations\"].items():\n",
    "    print(f\"\\n{eval_name}:\")\n",
    "    if \"weighted_score\" in result:\n",
    "        print(f\"  Score: {result['weighted_score']:.1f}/10\")\n",
    "    elif \"score\" in result:\n",
    "        print(f\"  Score: {result['score']:.1f}/10\")\n",
    "    if \"passed\" in result:\n",
    "        print(f\"  Passed: {result['passed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Email\n",
    "email = \"\"\"\n",
    "Hello Sarah,\n",
    "\n",
    "I hope this email finds you well. I wanted to reach out regarding the Q4 strategy meeting.\n",
    "Would you be available next Tuesday at 2 PM to discuss our market positioning and goals?\n",
    "\n",
    "Please let me know if this time works for you, or feel free to suggest an alternative.\n",
    "\n",
    "Best regards,\n",
    "John\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING EMAIL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "email_result = evaluator.evaluate_email(email)\n",
    "\n",
    "print(f\"\\nComposite Score: {email_result['composite_score']:.1f}/10\")\n",
    "print(f\"Status: {'✓ PASS' if email_result['passed'] else '✗ FAIL'}\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for eval_name, result in email_result[\"evaluations\"].items():\n",
    "    print(f\"\\n{eval_name}:\")\n",
    "    if \"weighted_score\" in result:\n",
    "        print(f\"  Score: {result['weighted_score']:.1f}/10\")\n",
    "    elif \"score\" in result:\n",
    "        print(f\"  Score: {result['score']:.1f}/10\")\n",
    "    if \"passed\" in result:\n",
    "        print(f\"  Passed: {result['passed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 7: Best Practices for Custom Evaluators\n",
    "\n",
    "Key principles for building effective evaluators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "BEST PRACTICES FOR CUSTOM EVALUATORS\n",
    "=\"*60)\n",
    "\n",
    "1. **Start Simple, Add Complexity**\n",
    "   - Begin with basic checks (length, required fields)\n",
    "   - Add LLM-based evaluation once baseline is working\n",
    "   - Avoid over-engineering until you have metrics\n",
    "\n",
    "2. **Match Evaluator Type to Measurement Need**\n",
    "   - Use CODE evaluators for objective, deterministic checks\n",
    "   - Use LLM-as-Judge for subjective, nuanced quality assessment\n",
    "   - Combine both for comprehensive evaluation\n",
    "\n",
    "3. **Provide Clear Rubrics for LLM-as-Judge**\n",
    "   - Write explicit criteria descriptions\n",
    "   - Use examples in prompts to anchor evaluations\n",
    "   - Test with multiple outputs to ensure consistency\n",
    "\n",
    "4. **Combine Multiple Evaluators**\n",
    "   - Use weighted scoring to reflect importance\n",
    "   - Balance structural (code) and quality (LLM) checks\n",
    "   - Set meaningful pass/fail thresholds\n",
    "\n",
    "5. **Iterate Based on Results**\n",
    "   - If evaluator scores don't match your judgment, refine criteria\n",
    "   - Track evaluator precision and recall\n",
    "   - Update rubrics as requirements change\n",
    "\n",
    "6. **Document Your Evaluation Logic**\n",
    "   - Explain why each criterion matters\n",
    "   - Record your weighting decisions\n",
    "   - Make thresholds explicit and defensible\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation in Netra\n",
    "\n",
    "Once you've defined your evaluators, integrate them with Netra's evaluation framework:\n",
    "\n",
    "1. **Create evaluators in dashboard** with your custom logic\n",
    "2. **Link to datasets** for automated evaluation\n",
    "3. **Track metrics** over time to measure quality trends\n",
    "4. **Compare configurations** using composite scores\n",
    "\n",
    "## Documentation Links\n",
    "\n",
    "- [Netra Documentation](https://docs.getnetra.ai)\n",
    "- [Custom Evaluators](https://docs.getnetra.ai/Evaluation/Custom-Evaluators)\n",
    "- [Evaluation Framework](https://docs.getnetra.ai/Evaluation)\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Evaluating RAG Quality](/Cookbooks/evaluation/evaluating-rag-quality) - RAG-specific evaluation\n",
    "- [A/B Testing Configurations](/Cookbooks/evaluation/ab-testing-configurations) - Compare model configurations\n",
    "- [Evaluating Agent Decisions](/Cookbooks/evaluation/evaluating-agent-decisions) - Agent behavior evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
