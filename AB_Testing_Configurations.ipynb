{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# A/B Testing Model Configurations\n",
    "\n",
    "This notebook shows you how to systematically compare AI configurations across different customer segments to optimize quality-cost tradeoffs. Learn when model upgrades, prompt changes, or parameter adjustments are worth the cost.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Set up tier configurations to test different models side-by-side\n",
    "- Run A/B tests comparing model performance on consistent datasets\n",
    "- Measure quality with LLM-as-Judge and code evaluators\n",
    "- Analyze cost vs. quality tradeoffs to make data-driven decisions\n",
    "- Compare configurations in the Netra dashboard\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.9+\n",
    "- OpenAI API key\n",
    "- Netra API key ([Get started here](https://docs.getnetra.ai/quick-start/Overview))\n",
    "- A test dataset with expected outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 0: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install netra-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"NETRA_API_KEY\"] = getpass(\"Enter your Netra API Key:\")\n",
    "os.environ[\"NETRA_OTLP_ENDPOINT\"] = getpass(\"Enter your Netra OTLP Endpoint:\")\n",
    "\n",
    "print(\"API keys configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Netra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netra import Netra\n",
    "from netra.instrumentation.instruments import InstrumentSet\n",
    "\n",
    "Netra.init(\n",
    "    app_name=\"ab-testing\",\n",
    "    headers=f\"x-api-key={os.getenv('NETRA_API_KEY')}\",\n",
    "    environment=\"testing\",\n",
    "    trace_content=True,\n",
    "    instruments={InstrumentSet.OPENAI},\n",
    ")\n",
    "\n",
    "print(\"Netra initialized for A/B testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Define Tier Configurations\n",
    "\n",
    "Set up different models and configurations to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class TierConfig:\n",
    "    \"\"\"Configuration for an A/B test tier.\"\"\"\n",
    "    name: str\n",
    "    model: str\n",
    "    temperature: float\n",
    "    description: str\n",
    "\n",
    "# Define tiers to compare\n",
    "TIERS = {\n",
    "    \"baseline\": TierConfig(\n",
    "        name=\"Baseline (GPT-4o-mini)\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        description=\"Current production configuration\"\n",
    "    ),\n",
    "    \"experimental_low_temp\": TierConfig(\n",
    "        name=\"Experimental (Lower Temperature)\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        description=\"Lower temperature for consistency\"\n",
    "    ),\n",
    "    \"premium\": TierConfig(\n",
    "        name=\"Premium (GPT-4o)\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        description=\"Higher quality model (more expensive)\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Tier configurations defined:\")\n",
    "for tier_id, config in TIERS.items():\n",
    "    print(f\"\\n{tier_id}:\")\n",
    "    print(f\"  Name: {config.name}\")\n",
    "    print(f\"  Model: {config.model}\")\n",
    "    print(f\"  Temperature: {config.temperature}\")\n",
    "    print(f\"  Description: {config.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Create Test Dataset\n",
    "\n",
    "Define consistent test cases with expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset for A/B testing\n",
    "TEST_CASES = [\n",
    "    {\n",
    "        \"id\": \"email-1\",\n",
    "        \"input\": \"Write a professional email asking for a meeting to discuss Q4 strategy.\",\n",
    "        \"expected_qualities\": [\"professional\", \"clear\", \"concise\", \"actionable\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"summary-1\",\n",
    "        \"input\": \"Summarize the key points: Machine learning is transforming industries by enabling systems to learn from data without explicit programming. Applications span healthcare, finance, transportation, and more. Key challenges include data quality, model interpretability, and regulatory compliance.\",\n",
    "        \"expected_qualities\": [\"accurate\", \"concise\", \"complete\", \"organized\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"creative-1\",\n",
    "        \"input\": \"Write a creative headline for a blog post about AI trends in 2024.\",\n",
    "        \"expected_qualities\": [\"engaging\", \"relevant\", \"unique\", \"readable\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"technical-1\",\n",
    "        \"input\": \"Explain how neural networks work in simple terms.\",\n",
    "        \"expected_qualities\": [\"accurate\", \"clear\", \"accessible\", \"thorough\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Test dataset: {len(TEST_CASES)} cases\")\n",
    "for case in TEST_CASES:\n",
    "    print(f\"  - {case['id']}: {case['input'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Implement Evaluation Function\n",
    "\n",
    "Create a simple quality evaluator using an LLM as a judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "class ABTestRunner:\n",
    "    \"\"\"Run A/B tests comparing different configurations.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.results = {}\n",
    "\n",
    "    def run_test_case(self, tier_id: str, test_case: dict) -> dict:\n",
    "        \"\"\"Run a single test case with a specific configuration.\"\"\"\n",
    "        config = TIERS[tier_id]\n",
    "        Netra.set_custom_attributes(key=\"tier_id\", value=tier_id)\n",
    "        Netra.set_custom_attributes(key=\"test_case_id\", value=test_case[\"id\"])\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": test_case[\"input\"]}],\n",
    "            temperature=config.temperature,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "        output = response.choices[0].message.content\n",
    "        tokens = response.usage.total_tokens\n",
    "\n",
    "        # Evaluate quality using a judge\n",
    "        quality_score = self.evaluate_output(\n",
    "            output,\n",
    "            test_case[\"expected_qualities\"]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"tier_id\": tier_id,\n",
    "            \"tier_name\": config.name,\n",
    "            \"test_case_id\": test_case[\"id\"],\n",
    "            \"output\": output,\n",
    "            \"tokens\": tokens,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"quality_score\": quality_score,\n",
    "        }\n",
    "\n",
    "    def evaluate_output(self, output: str, expected_qualities: list) -> float:\n",
    "        \"\"\"Evaluate output quality using LLM as a judge.\"\"\"\n",
    "        qualities_text = \", \".join(expected_qualities)\n",
    "        \n",
    "        judge_response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert evaluator. Rate the output on a scale of 1-10.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Rate this output for the following qualities: {qualities_text}\\n\\nOutput: {output[:500]}\\n\\nProvide only a number 1-10.\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=10,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(judge_response.choices[0].message.content.strip())\n",
    "            return min(max(score, 1), 10)  # Clamp between 1-10\n",
    "        except:\n",
    "            return 5.0  # Default if parsing fails\n",
    "\n",
    "    def store_result(self, result: dict):\n",
    "        \"\"\"Store test result.\"\"\"\n",
    "        key = f\"{result['tier_id']}_{result['test_case_id']}\"\n",
    "        self.results[key] = result\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary of A/B test results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"A/B Test Results Summary\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        # Aggregate by tier\n",
    "        tier_stats = {}\n",
    "        for result in self.results.values():\n",
    "            tier_id = result[\"tier_id\"]\n",
    "            if tier_id not in tier_stats:\n",
    "                tier_stats[tier_id] = {\n",
    "                    \"tier_name\": result[\"tier_name\"],\n",
    "                    \"quality_scores\": [],\n",
    "                    \"latencies\": [],\n",
    "                    \"tokens\": [],\n",
    "                }\n",
    "            tier_stats[tier_id][\"quality_scores\"].append(result[\"quality_score\"])\n",
    "            tier_stats[tier_id][\"latencies\"].append(result[\"latency_ms\"])\n",
    "            tier_stats[tier_id][\"tokens\"].append(result[\"tokens\"])\n",
    "\n",
    "        for tier_id, stats in sorted(tier_stats.items()):\n",
    "            avg_quality = sum(stats[\"quality_scores\"]) / len(stats[\"quality_scores\"])\n",
    "            avg_latency = sum(stats[\"latencies\"]) / len(stats[\"latencies\"])\n",
    "            avg_tokens = sum(stats[\"tokens\"]) / len(stats[\"tokens\"])\n",
    "\n",
    "            print(f\"\\n{stats['tier_name']}:\")\n",
    "            print(f\"  Avg Quality Score: {avg_quality:.2f}/10\")\n",
    "            print(f\"  Avg Latency: {avg_latency:.0f}ms\")\n",
    "            print(f\"  Avg Tokens/Call: {avg_tokens:.0f}\")\n",
    "\n",
    "\n",
    "print(\"A/B test runner class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Run the A/B Test\n",
    "\n",
    "Execute the test across all tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test runner\n",
    "runner = ABTestRunner()\n",
    "\n",
    "print(\"Starting A/B test execution...\\n\")\n",
    "\n",
    "# Run each tier on each test case\n",
    "for tier_id in TIERS.keys():\n",
    "    print(f\"\\nTesting tier: {TIERS[tier_id].name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for test_case in TEST_CASES:\n",
    "        print(f\"  Running {test_case['id']}...\", end=\"\", flush=True)\n",
    "        result = runner.run_test_case(tier_id, test_case)\n",
    "        runner.store_result(result)\n",
    "        print(f\" âœ“ (Quality: {result['quality_score']:.1f}/10)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"A/B test execution complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "runner.print_summary()\n",
    "\n",
    "# Print detailed results by test case\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Detailed Results by Test Case\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for test_case in TEST_CASES:\n",
    "    print(f\"\\n{test_case['id']}: {test_case['input']}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for tier_id in TIERS.keys():\n",
    "        key = f\"{tier_id}_{test_case['id']}\"\n",
    "        if key in runner.results:\n",
    "            result = runner.results[key]\n",
    "            print(f\"\\n  {result['tier_name']}:\")\n",
    "            print(f\"    Quality: {result['quality_score']:.1f}/10\")\n",
    "            print(f\"    Latency: {result['latency_ms']:.0f}ms\")\n",
    "            print(f\"    Tokens: {result['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 8: Cost vs. Quality Analysis\n",
    "\n",
    "Determine which configuration offers the best cost-quality tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate costs (rough pricing for illustration)\n",
    "MODEL_COSTS = {\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60}  # per 1M tokens\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Cost-Quality Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Aggregate stats by tier\n",
    "tier_stats = {}\n",
    "for result in runner.results.values():\n",
    "    tier_id = result[\"tier_id\"]\n",
    "    if tier_id not in tier_stats:\n",
    "        tier_stats[tier_id] = {\n",
    "            \"tier_name\": result[\"tier_name\"],\n",
    "            \"model\": TIERS[tier_id].model,\n",
    "            \"quality_scores\": [],\n",
    "            \"tokens\": [],\n",
    "        }\n",
    "    tier_stats[tier_id][\"quality_scores\"].append(result[\"quality_score\"])\n",
    "    tier_stats[tier_id][\"tokens\"].append(result[\"tokens\"])\n",
    "\n",
    "# Calculate and display cost-quality analysis\n",
    "for tier_id, stats in sorted(tier_stats.items()):\n",
    "    avg_quality = sum(stats[\"quality_scores\"]) / len(stats[\"quality_scores\"])\n",
    "    avg_tokens = sum(stats[\"tokens\"]) / len(stats[\"tokens\"])\n",
    "    \n",
    "    model = stats[\"model\"]\n",
    "    if model in MODEL_COSTS:\n",
    "        pricing = MODEL_COSTS[model]\n",
    "        cost_per_call = (avg_tokens * 0.7 * pricing[\"input\"] + avg_tokens * 0.3 * pricing[\"output\"]) / 1_000_000\n",
    "    else:\n",
    "        cost_per_call = 0\n",
    "    \n",
    "    quality_per_dollar = avg_quality / max(cost_per_call, 0.001)\n",
    "    \n",
    "    print(f\"\\n{stats['tier_name']}:\")\n",
    "    print(f\"  Model: {model}\")\n",
    "    print(f\"  Avg Quality: {avg_quality:.2f}/10\")\n",
    "    print(f\"  Avg Tokens: {avg_tokens:.0f}\")\n",
    "    print(f\"  Est. Cost/Call: ${cost_per_call:.4f}\")\n",
    "    print(f\"  Quality per Dollar: {quality_per_dollar:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Recommendation: Choose the configuration with highest quality/cost ratio\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dashboard Comparison\n",
    "\n",
    "In the Netra dashboard, you can:\n",
    "\n",
    "1. **Filter by tier_id** to see results per configuration\n",
    "2. **Compare latency** between tiers for the same test case\n",
    "3. **Track token usage** to calculate actual costs\n",
    "4. **Analyze quality scores** from your evaluators\n",
    "\n",
    "## Documentation Links\n",
    "\n",
    "- [Netra Documentation](https://docs.getnetra.ai)\n",
    "- [Evaluation Framework](https://docs.getnetra.ai/Evaluation)\n",
    "- [Evaluators Guide](https://docs.getnetra.ai/Evaluation/Evaluators)\n",
    "- [Test Runs](https://docs.getnetra.ai/Evaluation/Test-Runs)\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Evaluating RAG Quality](/Cookbooks/evaluation/evaluating-rag-quality) - Quality evaluation for RAG systems\n",
    "- [Custom Evaluator Patterns](/Cookbooks/evaluation/custom-evaluator-patterns) - Build domain-specific evaluators\n",
    "- [Evaluating Agent Decisions](/Cookbooks/evaluation/evaluating-agent-decisions) - Evaluate agent behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
