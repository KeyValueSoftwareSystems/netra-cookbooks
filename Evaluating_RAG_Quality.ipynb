{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluating RAG Quality\n",
    "\n",
    "This notebook shows you how to **systematically evaluate** the quality of a Retrieval-Augmented Generation (RAG) pipeline using Netra's evaluation framework. You'll learn to measure retrieval effectiveness, answer accuracy, and detect hallucinations.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Build comprehensive test datasets with expected answers to benchmark your RAG system\n",
    "- Configure LLM-as-Judge evaluators for retrieval quality, answer correctness, and faithfulness\n",
    "- Execute systematic test runs and collect metrics across your entire dataset\n",
    "- Interpret results, identify failure patterns, and improve your pipeline\n",
    "\n",
    "**Prerequisites:**\n",
    "- A RAG pipeline with Netra tracing configured (see Tracing RAG Pipeline cookbook)\n",
    "- Python >=3.10, <3.14\n",
    "- Netra API key ([Get started here](https://docs.getnetra.ai/quick-start/Overview))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 0: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install netra-sdk openai chromadb pypdf reportlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"NETRA_API_KEY\"] = getpass(\"Enter your Netra API Key:\")\n",
    "os.environ[\"NETRA_OTLP_ENDPOINT\"] = getpass(\"Enter your Netra OTLP Endpoint:\")\n",
    "\n",
    "print(\"API keys configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Netra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netra import Netra\n",
    "from netra.instrumentation.instruments import InstrumentSet\n",
    "\n",
    "# Initialize Netra\n",
    "Netra.init(\n",
    "    app_name=\"rag-evaluation\",\n",
    "    headers=f\"x-api-key={os.getenv('NETRA_API_KEY')}\",\n",
    "    environment=\"evaluation\",\n",
    "    trace_content=True,\n",
    "    instruments={\n",
    "        InstrumentSet.OPENAI,\n",
    "        InstrumentSet.CHROMA,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Netra initialized for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Create or Import Your RAG Pipeline\n",
    "\n",
    "You can use the PDFChatbot from the Tracing RAG Pipeline cookbook. Import or define your RAG pipeline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll create a minimal RAG pipeline\n",
    "# In production, import your actual chatbot class from the Tracing RAG Pipeline cookbook\n",
    "\n",
    "import uuid\n",
    "from typing import List, Dict, Optional\n",
    "from pypdf import PdfReader\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "\n",
    "def load_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=texts\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def retrieve_chunks(query: str, collection, top_k: int = 3) -> List[dict]:\n",
    "    \"\"\"Retrieve relevant chunks for a query.\"\"\"\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    retrieved = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        similarity = 1 - results[\"distances\"][0][i]\n",
    "        retrieved.append({\n",
    "            \"content\": doc,\n",
    "            \"similarity_score\": similarity\n",
    "        })\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "def generate_answer(query: str, context_chunks: List[dict]) -> dict:\n",
    "    \"\"\"Generate an answer using the retrieved context.\"\"\"\n",
    "    context = \"\\n\\n\".join([chunk[\"content\"] for chunk in context_chunks])\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that answers questions based on the provided context. Only use information from the context to answer. If the answer is not in the context, say so.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"token_usage\": {\n",
    "            \"prompt\": response.usage.prompt_tokens,\n",
    "            \"completion\": response.usage.completion_tokens,\n",
    "            \"total\": response.usage.total_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "class PDFChatbot:\n",
    "    \"\"\"RAG Pipeline with Netra tracing.\"\"\"\n",
    "\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.collection = None\n",
    "        self.chunks: List[str] = []\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the vector store with PDF content.\"\"\"\n",
    "        pdf_text = load_pdf(self.pdf_path)\n",
    "        self.chunks = chunk_text(pdf_text)\n",
    "        embeddings = generate_embeddings(self.chunks)\n",
    "\n",
    "        self.collection = chroma_client.create_collection(name=f\"pdf_{self.session_id[:8]}\")\n",
    "        self.collection.add(\n",
    "            documents=self.chunks,\n",
    "            embeddings=embeddings,\n",
    "            ids=[f\"chunk_{i}\" for i in range(len(self.chunks))]\n",
    "        )\n",
    "        print(f\"RAG pipeline initialized with {len(self.chunks)} chunks\")\n",
    "\n",
    "    def chat(self, query: str, user_id: Optional[str] = None) -> dict:\n",
    "        \"\"\"Process a chat message and return the response.\"\"\"\n",
    "        Netra.set_session_id(self.session_id)\n",
    "        if user_id:\n",
    "            Netra.set_user_id(user_id)\n",
    "\n",
    "        retrieved_chunks = retrieve_chunks(query, self.collection)\n",
    "        result = generate_answer(query, retrieved_chunks)\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"retrieved_chunks\": retrieved_chunks,\n",
    "            \"token_usage\": result[\"token_usage\"]\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"RAG pipeline class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Create a Test Dataset\n",
    "\n",
    "Start by building a dataset of question-answer pairs that represent real usage patterns. You can:\n",
    "\n",
    "1. **Create from Traces** - Go to Traces in the Netra dashboard, find good question-answer pairs, and click \"Add to Dataset\"\n",
    "2. **Create from Dashboard** - Go to Evaluation → Datasets, click \"Create Dataset\", and add test cases\n",
    "3. **Create Programmatically** - Use this notebook to create test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Define a test dataset programmatically\n",
    "# In production, you'll get this from the Netra dashboard\n",
    "\n",
    "test_dataset = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning?\",\n",
    "        \"expected_output\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the three types of machine learning?\",\n",
    "        \"expected_output\": \"The three main types are: 1) Supervised Learning - learning from labeled data, 2) Unsupervised Learning - learning patterns from unlabeled data, 3) Reinforcement Learning - learning through interaction with an environment.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who coined the term machine learning?\",\n",
    "        \"expected_output\": \"Arthur Samuel coined the term 'machine learning' in 1959 while at IBM.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is an application of machine learning?\",\n",
    "        \"expected_output\": \"Machine learning has numerous applications including image recognition, natural language processing, recommendation systems, autonomous vehicles, healthcare diagnostics, and fraud detection.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is not mentioned in the document?\",\n",
    "        \"expected_output\": \"The information about quantum computing is not mentioned in the document.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created test dataset with {len(test_dataset)} test cases\")\n",
    "print(\"\\nExample test case:\")\n",
    "print(f\"Query: {test_dataset[0]['query']}\")\n",
    "print(f\"Expected: {test_dataset[0]['expected_output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Define Evaluators\n",
    "\n",
    "Create evaluators in the Netra dashboard under **Evaluation → Evaluators → Add Evaluator**. For RAG pipelines, we recommend:\n",
    "\n",
    "1. **Context Relevance** - Checks if retrieved chunks contain relevant information (score >= 0.7)\n",
    "2. **Answer Correctness** - Compares generated answer against expected answer (score >= 0.7)\n",
    "3. **Faithfulness** - Verifies answer is grounded in retrieved context (score >= 0.8)\n",
    "\n",
    "For now, we'll show how to structure your evaluation once evaluators are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator configuration reference\n",
    "evaluators_reference = {\n",
    "    \"context_relevance\": {\n",
    "        \"template\": \"Context Relevance\",\n",
    "        \"output_type\": \"Numerical\",\n",
    "        \"pass_criteria\": \"score >= 0.7\",\n",
    "        \"purpose\": \"Checks whether retrieved chunks contain information relevant to answering the question\"\n",
    "    },\n",
    "    \"answer_correctness\": {\n",
    "        \"template\": \"Answer Correctness\",\n",
    "        \"output_type\": \"Numerical\",\n",
    "        \"pass_criteria\": \"score >= 0.7\",\n",
    "        \"purpose\": \"Compares the generated answer against the expected answer\"\n",
    "    },\n",
    "    \"faithfulness\": {\n",
    "        \"template\": \"Faithfulness\",\n",
    "        \"output_type\": \"Numerical\",\n",
    "        \"pass_criteria\": \"score >= 0.8\",\n",
    "        \"purpose\": \"Verifies that the answer is grounded in the retrieved context\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Evaluator reference:\")\n",
    "for name, config in evaluators_reference.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Template: {config['template']}\")\n",
    "    print(f\"  Purpose: {config['purpose']}\")\n",
    "    print(f\"  Pass Criteria: {config['pass_criteria']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Create Sample PDF and Initialize Chatbot\n",
    "\n",
    "For this example, we'll create a sample PDF with machine learning content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_sample_pdf(filename: str = \"sample_document.pdf\"):\n",
    "    \"\"\"Create a sample PDF for testing.\"\"\"\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Page 1: Introduction\n",
    "    c.setFont(\"Helvetica-Bold\", 24)\n",
    "    c.drawString(100, height - 100, \"Introduction to Machine Learning\")\n",
    "\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    text = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that enables systems to learn\n",
    "    and improve from experience without being explicitly programmed. The term was coined\n",
    "    by Arthur Samuel in 1959 while at IBM.\n",
    "\n",
    "    Machine learning algorithms build a mathematical model based on sample data, known as\n",
    "    training data, in order to make predictions or decisions without being explicitly\n",
    "    programmed to perform the task.\n",
    "\n",
    "    The primary aim is to allow computers to learn automatically without human intervention\n",
    "    or assistance and adjust actions accordingly.\n",
    "    \"\"\"\n",
    "\n",
    "    y = height - 150\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        c.drawString(100, y, line.strip())\n",
    "        y -= 20\n",
    "\n",
    "    # Page 2: Types of ML\n",
    "    c.showPage()\n",
    "    c.setFont(\"Helvetica-Bold\", 18)\n",
    "    c.drawString(100, height - 100, \"Types of Machine Learning\")\n",
    "\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    text = \"\"\"\n",
    "    There are three main types of machine learning:\n",
    "\n",
    "    1. Supervised Learning: The algorithm learns from labeled training data and makes\n",
    "       predictions based on that data. Examples include classification and regression.\n",
    "\n",
    "    2. Unsupervised Learning: The algorithm learns patterns from unlabeled data without\n",
    "       any guidance. Examples include clustering and dimensionality reduction.\n",
    "\n",
    "    3. Reinforcement Learning: The algorithm learns through interaction with an environment,\n",
    "       receiving rewards or penalties for actions. Used in robotics and game playing.\n",
    "\n",
    "    Each type has its own applications and is suited for different kinds of problems.\n",
    "    \"\"\"\n",
    "\n",
    "    y = height - 150\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        c.drawString(100, y, line.strip())\n",
    "        y -= 20\n",
    "\n",
    "    # Page 3: Applications\n",
    "    c.showPage()\n",
    "    c.setFont(\"Helvetica-Bold\", 18)\n",
    "    c.drawString(100, height - 100, \"Applications of Machine Learning\")\n",
    "\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    text = \"\"\"\n",
    "    Machine learning has numerous real-world applications:\n",
    "\n",
    "    - Image Recognition: Identifying objects, faces, and scenes in images\n",
    "    - Natural Language Processing: Translation, sentiment analysis, chatbots\n",
    "    - Recommendation Systems: Netflix, Amazon, Spotify recommendations\n",
    "    - Autonomous Vehicles: Self-driving cars use ML for navigation\n",
    "    - Healthcare: Disease diagnosis, drug discovery, personalized treatment\n",
    "    - Financial Services: Fraud detection, algorithmic trading, credit scoring\n",
    "\n",
    "    The field continues to grow rapidly with new applications emerging regularly.\n",
    "    \"\"\"\n",
    "\n",
    "    y = height - 150\n",
    "    for line in text.strip().split(\"\\n\"):\n",
    "        c.drawString(100, y, line.strip())\n",
    "        y -= 20\n",
    "\n",
    "    c.save()\n",
    "    print(f\"Created sample PDF: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Create the sample PDF and initialize chatbot\n",
    "pdf_path = create_sample_pdf()\n",
    "chatbot = PDFChatbot(pdf_path)\n",
    "chatbot.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Run Evaluation on Test Dataset\n",
    "\n",
    "Execute your RAG pipeline against each test case in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RAG pipeline on each test case\n",
    "print(\"=\"*60)\n",
    "print(\"Running Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for i, test_case in enumerate(test_dataset, 1):\n",
    "    query = test_case[\"query\"]\n",
    "    expected = test_case[\"expected_output\"]\n",
    "    \n",
    "    # Run the chatbot\n",
    "    response = chatbot.chat(query, user_id=\"eval-user\")\n",
    "    \n",
    "    result = {\n",
    "        \"test_case\": i,\n",
    "        \"query\": query,\n",
    "        \"expected\": expected,\n",
    "        \"actual\": response[\"answer\"],\n",
    "        \"retrieved_chunks\": len(response[\"retrieved_chunks\"]),\n",
    "        \"max_similarity\": max(c[\"similarity_score\"] for c in response[\"retrieved_chunks\"]),\n",
    "        \"token_usage\": response[\"token_usage\"][\"total\"]\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\n--- Test Case {i} ---\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Retrieved {result['retrieved_chunks']} chunks (max similarity: {result['max_similarity']:.3f})\")\n",
    "    print(f\"Answer: {response['answer'][:100]}...\")\n",
    "    print(f\"Tokens: {result['token_usage']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluation complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Results\n",
    "\n",
    "Review the evaluation results and identify patterns in failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"\\nEvaluation Results Summary:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_tokens = sum(r[\"token_usage\"] for r in results)\n",
    "avg_chunks = sum(r[\"retrieved_chunks\"] for r in results) / len(results)\n",
    "avg_similarity = sum(r[\"max_similarity\"] for r in results) / len(results)\n",
    "\n",
    "print(f\"Total test cases: {len(results)}\")\n",
    "print(f\"Total tokens used: {total_tokens}\")\n",
    "print(f\"Average chunks retrieved per query: {avg_chunks:.1f}\")\n",
    "print(f\"Average max similarity score: {avg_similarity:.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "for result in results:\n",
    "    print(f\"\\nTest {result['test_case']}: {result['query'][:40]}...\")\n",
    "    print(f\"  Similarity: {result['max_similarity']:.3f}\")\n",
    "    print(f\"  Chunks: {result['retrieved_chunks']}\")\n",
    "    print(f\"  Tokens: {result['token_usage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Step 9: Using the Netra Dashboard for Full Evaluation\n",
    "\n",
    "For complete evaluation with LLM-as-Judge scoring:\n",
    "\n",
    "1. Create a dataset in the Netra dashboard (Evaluation → Datasets)\n",
    "2. Configure evaluators (Evaluation → Evaluators)\n",
    "3. Run test suite: Get your dataset ID and use the API below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation using Netra's evaluation API\n",
    "# First, create a dataset in the dashboard and get its ID\n",
    "\n",
    "def run_evaluation_suite(dataset_id: str):\n",
    "    \"\"\"\n",
    "    Run evaluation using your dataset and evaluators from Netra dashboard.\n",
    "    \n",
    "    Steps:\n",
    "    1. Go to Netra dashboard → Evaluation → Datasets\n",
    "    2. Create a dataset and get its ID\n",
    "    3. Go to Evaluation → Evaluators and configure evaluators\n",
    "    4. Pass dataset_id below to run the test suite\n",
    "    \"\"\"\n",
    "    print(f\"\\nTo run full evaluation:\")\n",
    "    print(f\"1. Replace 'dataset_id' with your actual dataset ID from Netra\")\n",
    "    print(f\"2. Uncomment the code below\")\n",
    "    print(f\"\\n# Uncomment to run:\")\n",
    "    print(f\"# dataset = Netra.evaluation.get_dataset(dataset_id)\")\n",
    "    print(f\"# Netra.evaluation.run_test_suite(\")\n",
    "    print(f\"#     name='RAG Quality Evaluation',\")\n",
    "    print(f\"#     data=dataset,\")\n",
    "    print(f\"#     task=lambda eval_input: chatbot.chat(\")\n",
    "    print(f\"#         query=eval_input['query'],\")\n",
    "    print(f\"#         user_id='eval-user'\")\n",
    "    print(f\"#     )['answer']\")\n",
    "    print(f\"# )\")\n",
    "    print(f\"#\")\n",
    "    print(f\"# print('Evaluation complete! View results in the Netra dashboard.')\")\n",
    "    print(f\"# Netra.shutdown()\")\n",
    "\n",
    "run_evaluation_suite(\"your-dataset-id-from-netra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 10: Interpreting Evaluation Scores\n",
    "\n",
    "When analyzing your evaluation results, look for patterns:\n",
    "\n",
    "| Low Score In | Likely Cause | How to Fix |\n",
    "|--------------|--------------|----------|\n",
    "| **Context Relevance** | Wrong chunks retrieved | Increase `top_k`, reduce chunk size, add overlap |\n",
    "| **Answer Correctness** | LLM misinterprets context | Improve system prompt, lower temperature |\n",
    "| **Faithfulness** | Model hallucinates | Add explicit grounding instructions, use stronger model |\n",
    "\n",
    "Click **View Trace** on failed test cases to see:\n",
    "- The exact chunks that were retrieved\n",
    "- Similarity scores for each chunk\n",
    "- The full prompt sent to the LLM\n",
    "- Token usage and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Continuous Evaluation Strategy\n",
    "\n",
    "For production RAG systems, run evaluations regularly:\n",
    "\n",
    "1. **On every deployment** — Run your test suite in CI/CD before releasing changes\n",
    "2. **Weekly benchmarks** — Track quality trends over time\n",
    "3. **After prompt changes** — Measure the impact of system prompt modifications\n",
    "4. **After parameter tuning** — Validate that chunk size or top_k changes improve quality\n",
    "\n",
    "## Documentation Links\n",
    "\n",
    "- [Netra Documentation](https://docs.getnetra.ai)\n",
    "- [Evaluation API](https://docs.getnetra.ai/Evaluation)\n",
    "- [Datasets Guide](https://docs.getnetra.ai/Evaluation/Datasets)\n",
    "- [Evaluators Guide](https://docs.getnetra.ai/Evaluation/Evaluators)\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Trace Your RAG Pipeline](/Cookbooks/observability/tracing-rag-pipeline) - Add comprehensive tracing\n",
    "- [Custom Evaluator Patterns](/Cookbooks/evaluation/custom-evaluator-patterns) - Build domain-specific evaluators\n",
    "- [AB Testing Configurations](/Cookbooks/evaluation/ab-testing-configurations) - Compare different pipeline configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
